# 11 生成式对抗网络（GAN）

## 11.1 GAN概述

GAN的全称是Generative adversarial network，中文翻译过来就是对抗式神经网络。

对抗神经网络其实是两个网络的组合，可以理解为一个网络生成模拟数据**（生成网络Generator）**，另一个网络判断生成的数据是真实的还是模拟的**（判别网络Discriminator）**。

生成网络要不断优化自己生成的数据让判别网络判断不出来，判别网络也要优化自己让自己判断得更准确。二者关系形成对抗，因此叫对抗神经网络。

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714105354101.png" alt="image-20240714105354101" style="zoom: 67%;" />

特别之处：

输入network的，除了$x$以外还有一个从某一简单的（已知）分布中随机采样得到的一个**random的variable $z$**

**分布：**高斯分布、均一分布

**方法：**

- X,Z两个向量直接接起来,变成一个比较长的向量,作为network的input
- X跟Z正好长度一样，相加以后,当做network的input

从network输出的也将是一个分布。





**为什么要分布？让机器具有Creativity**

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714125137888.png" alt="image-20240714125137888" style="zoom: 50%;" />

对于这个地图的例子，给了几个画面进入Network，输出下一步的画面，但是在墙角需要转弯的时候，有的时候需要左转，有的时候需要右转，但是同时转弯必定是不可能的。

但是可以加入一个distribution

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714125327397.png" alt="image-20240714125327397" style="zoom:50%;" />

此时可以输出一个关于结果的分布，如果结果是二分的话，也就是只能由左转和右转其中一种可能、

**加入Disribution主要是为了增加创造力（Creativity）**

比如画画和聊天机器人

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714125551926.png" alt="image-20240714125551926" style="zoom: 50%;" />

## 11.2 GNN 基础构成

### 11.2.1 生成器Generator



<img src="../assets/11.生成式对抗网络（GAN）/image-20240714125804931.png" alt="image-20240714125804931" style="zoom:50%;" />

这里使用的是简化的Unconditional generation，也就是现在不考虑原输入x,只是将Normal distribution作为输入

**<img src="../assets/11.生成式对抗网络（GAN）/image-20240714125925534.png" alt="image-20240714125925534" style="zoom:67%;" />**



$z$是从一个normal distribution里sample出来的向量,通常会是一个low-dimensional的向量

一张图片就是一个非常高维的向量,所以generator实际上做的事情,就是产生一个非常高维的向量

当你输入的向量不同的时候,你的输出就会跟著改变.

**这里的Generator主要目的就是把低维分布转化为高维分布。**

### 11.2.2 判别器Discriminator

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714130138042.png" alt="image-20240714130138042" style="zoom:50%;" />

例如，输入是一个图片，discriminato判断生成的数据是真实的还是模拟了，指标为**Scalar**,该值越大代表越真实，越小代表越假（e.g.越像是真实的二次元人物的图像)。

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714130313307.png" alt="image-20240714130313307" style="zoom:50%;" />

其中生成器和判别器都是可以自行设定的，可以用CNN也可以用transformer

## 11.3 GAN的基础思想



<img src="../assets/11.生成式对抗网络（GAN）/image-20240714130538533.png" alt="image-20240714130538533" style="zoom:50%;" />

Generator相当于一个不断进化的伪装，逃避Discriminator的识别

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714130745765.png" alt="image-20240714130745765" style="zoom:50%;" />

## 11.4 GAN算法实现

### **Setp 0：**初始化generator与discriminator。

开始以下迭代：

### **Step 1: **Fix generator G, and update discriminator D

![image-20240714131659474](../assets/11.生成式对抗网络（GAN）/image-20240714131659474.png)

- generator的参数是随机初始化的，从这个高斯分布中,去random sample一堆vector丢到generator裡面，出一些图片会跟正常的二次元人物非常的不像。
- 对于discriminator来说,这就是一个**分类**的问题,或者是**回归**的问题。需要拿真正的二次元人物头像,跟generator产生出来的结果,去训练你的discriminator,**分辨他们的差异**：
  - 如果视作分类的问题,你就把真正的人脸当作类别1,Generator产生出来的,这些图片当作类别2,然后训练一个classifier
  - 它当作regression的问题,那你就教你的discriminator,看到真实二次元人脸图片就输出1,看到生成的图片你就输出0

### **Step 2: **Fix discriminator D, and update generator G

也就是让Generator生成的图片骗过Discriminator

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714132133304.png" alt="image-20240714132133304" style="zoom: 50%;" />

固定住判别器，训练Generator，**目标是Discriminator的输出值越大越好**⇒假设Discriminator已经能够分辨照片的真假，让它觉得是真正的图片的话,那generator產生出来的图片,可能就可以以假乱真

**另一个角度:**

把generator跟Discriminator直接接起来,**当做一个比较大的network来看待**,通过调整前几层Generator部分的参数，使得整个模型的输出分数越大越好。中间的hidden layer相当于是一个图片的暂存器。

**用gradient ascent，调整generator，让目标函数越大越好。**

**这里为什么先固定D,后固定G?**

因为在step2中要对G进行调整，但是调整过程中要用到D输出的scalar来进行调参，若后固定G，则只要靠近scalar的参数调整一个比较大的数就可以实现，从而导致结果失效，



### **Step 3：**反复进行step1 和step2

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714132918936.png" alt="image-20240714132918936" style="zoom:50%;" />

## 11.5 GAN实例

#### Anime Face Generation（动画人脸生成）

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714133423958.png" alt="image-20240714133423958" style="zoom:50%;" />

**Progressive GAN**——真实人脸生成⇒生成“没有看过的&连续变化的”人脸

generator,就是输入一个向量 输出一张图片。把输入的向量,做内插interpolation（中间插值），看到两张图片之间连续的变化。

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714133545571.png" alt="image-20240714133545571" style="zoom: 50%;" />

## 11.6 GAN的理论基础

### 11.6.1 GAN目标

Objective目标：让**生成器**产生的与真实数据之间的分布接近



<img src="../assets/11.生成式对抗网络（GAN）/image-20240714133858547.png" alt="image-20240714133858547" style="zoom:50%;" />

有一个 Generator

- 给它一大堆的 Vector,给它从 Normal Distribution 采样得到
- 丢进这个 Generator 以后,会產生一个比较复杂的 Distribution,这个复杂 Distribution,我们叫它$P_G$
- 有一堆的 **真正的 Data** 也形成了另外一个 Distribution,叫做$P_{data}$ ,我们期待$P_G$ **跟 $P_{data}$​ 越接近越好**



GAN的目标本质上就是找一个G（实际就是找一组G的参数，这其实也是Generator的目标），使得Pg和Pdata之间的散度越小越好



### **11.6.2 GAN巧妙之处-计算Divergence**

#### Sample采样

$$
G^*=arg\min_GDiv(P_G,P_{data})
$$

虽然不知道Pg和Pdata的分布，但只要我们能够采样，就有办法评估两个分布之间的差异



<img src="../assets/11.生成式对抗网络（GAN）/image-20240714134413173.png" alt="image-20240714134413173" style="zoom:67%;" />

衡量分布之间的距离⇒Divergence→通过**sample数据**，借助Discriminator来计算.

注意这里是对**生成器**产生的数据于**真实数据**分布作对比。只不过利用Sample数据，通过对两个sample数据组中的数据进行计算Divergence。那么如何实现计算距离？？？



我们前面谈到训练的步骤是先固定G,训练D.再固定D,训练G,使得最终输出的结果scalar最大，从而使得生成器输出的效果接近真实效果。

同样现在我们要计算**生成器**产生的数据与**真实数据**分布的Divergence。

**我们可以固定G，然后训练D。找到一个很好的D,能够分别出$P_{data}$和$P_D$的差别（或者说量化差别）。**

如何通过D量化两个样本的差别？？

#### 利用Discriminator量化样本差别

- 训练Discriminator，希望：
  - 看到 **Real Data**,就给它比较**高的分数**
  - 看到这个 **Generative 的 Data**,就给它比较**低的分数**

![img](../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1ea483d5-18a3-4437-8a75-31dacdb3801d%2FUntitled.png)

- 写成公式：

  $$\text{Training:}D^*=arg\max_DV(D,G)$$​

  $\begin{aligned}\text{Objective Function for D}\\V(G,D)&=E_{y\sim P_{data}}[logD(y)]+E_{y\sim P_{G}}[log\bigl(1-D(y)\bigr)]\end{aligned}$

  - $V$:对于从 $P_{data}$ 裡面 Sample 出来的真正的 Image，希望他的评分越高越好；从$P_G$也就是 Generato 所產生出来的图片，希望评分越低越好。

  - $V$式子经过推导可以发现，他**与JS散度有关**，可以**衡量两个分布之间的距离**。

    

  #### JS 散度

  

  关于JS散度详细看：[KL散度、JS散度、Wasserstein距离](https://www.cnblogs.com/Renyi-Fan/p/13494662.html)

  [【DL经典回顾】距离度量大汇总（19-JS散度(Jensen-Shannon divergence)）](https://blog.csdn.net/qq_42983182/article/details/137498285)

- 训练过程等价于训练一个**二元分类器**。

  ![img](../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1f0f5154-22fc-4bce-8a00-e20e5c328a57%2FUntitled.png)

  - $V$的形式等价于Cross entropy乘上负号。

<img src="../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fd83e3f02-e018-4184-9f34-b73673d23ed4%2FUntitled.png" alt="img" style="zoom:50%;" />

直观理解：

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F3ece000c-13f9-4412-a491-f13d056a681e%2FUntitled.png?table=block&id=a750ff19-b25e-42bd-babf-602dbf8b308e&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=1380&userId=&cache=v2" alt="img" style="zoom: 33%;" />

- **两组 Data （$P_G$ 跟 $P_{data}$ ）很像**,它们差距没有很大。D目标函数的最大值小，表示D很难把这两个分布分开，进而表示散度越小。这个时候Discriminator 就是在 Train 一个Binary Classifier,**这个问题很难,所以你在解这个 Optimization Problem 的时候,你就没有办法让二元分类器训练任务的损失函数（交叉熵）很小**，也就是没有办法让Objective 的值$V$非常大⇒小的 Divergence，对应$V$ 的Maximum 的值就比较小。
- **两组 Data （$P_G$ 跟 $P_{data}$ ）很不像，Divergence值很大**，此时这个二元分类器就可以很轻易地分开，训练的损失函数很小，也就是目标函数$V$的值很大。⇒大的 Divergence，对应$V$ 的Maximum 的值就比较大

**结论：用$max_D V(D,G)$代替$Div(D,G)$​**

解释了generator 与 discriminator之间纠缠的关系

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714135311193.png" alt="image-20240714135311193" style="zoom:50%;" />

### 11.6.3 其他Divergence



不同的Obejctive 与 Divergence之间的对应关系

![img](../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fedaf0df3-ebac-4333-861f-2cb60f66bce8%2FUntitled.png)

## 11.7 GAN训练技巧-WGAN

### 11.7.1 样本特性



#### 1-生成的分布$P_G$和真实数据的分布$P_{data}$重叠的部分非常小

JS divergence is not suitable

![image-20240714162226434](../assets/11.生成式对抗网络（GAN）/image-20240714162226434.png)

PG 跟 Pdata,它们都是要產生图片的。图片是高维空间裡面的一个**非常狭窄的**低维的 Manifold【流形（英语：Manifolds）是可以局部欧几里得空间化的一个拓扑空间，是欧几里得空间中的曲线、曲面等概念的推广】高维空间中随便 Sample 一个点都不是图片只有非常小的范围,Sample 出来会是图片。因而，这部分可以被sample出二次元头像的部分，**可以理解为二维空间中的一条线。**

进而，PG 跟 Pdata,它们都是 Low Dimensional 的 Manifold,除非它刚好重合,**不然它们相交的范围,几乎是可以忽略的。**





#### 2-基于Sample的Divergence衡量具有局限性

难以采样到“重叠的点”

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714162359570.png" alt="image-20240714162359570" style="zoom: 67%;" />



由于我们无法得知 $P_G$与$P_{data}$的真实情况，只能通过采样来进行“推断”。对于 Discriminator 来说，如果 Sample 的点不够多、不够密,它那就算这两个 Distribution 实际上有重叠，也无法判断出来。

⇒可以事实上“划出一条线”，使得采样点毫无联系。

### 11.7.2 JS Divergence 缺点

**JS Divergence 的特性：两个没有重叠的分布,JS Divergence 算出来的值恒为 Log2**，无法衡量这类（没有重叠的）分布之间的关系。



Pg和Pdata往往重叠的部分很少。

1. 第一个理由：比如图片是高维空间的低维manifold，只有非常小的范围才能表示有效的图片。
2. 第二个理由，即使两个分布有重叠，如果sample的点不够多，对D来说也是不重叠的

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714161322821.png" alt="image-20240714161322821" style="zoom:50%;" />

****

中间的比左边的好，但JS散度值却都是log2。**问题：根据理论，这个 Discriminator 的 Loss，也就是Binary Classifier Loss越来越大时，代表问题越来越难,代表我们的 Generated Data,跟 Real 的 Data 越来越接近。但是Binary Classifier 训练完总是可以让它的正确率变到 100%， Loss没有什麼意义⇒两个分布始终很难接近**。

过去，人眼观察。





### 11.7.3 Wasserstein Distance

Wasserstein Distance直观理解：“推土”距离的最小值⇒穷举/优化问题



<img src="../assets/11.生成式对抗网络（GAN）/v2-bc3d597a5645118d35bb984a3c886521_720w.webp" alt="img" style="zoom: 67%;" />

用将一堆土P移到Q的位置的距离衡量两个分布P与Q之间的差异。

基于JS散度的问题，使得人们提出用Wasserstein distance，即W-GAN



Wasserstein Distance的作用：能够分辨出没有重叠Divergence的差距从而实现两个分布从毫不相关到逐渐接近

![image-20240714162851003](../assets/11.生成式对抗网络（GAN）/image-20240714162851003-1720945733025-15.png)

把P塑造成Q的方法有很多种（moving plans），用不同的moving plans有不同的平均举例，所以要**穷举**所有的moving plans，挑最小的举例作为散度

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714162940260.png" alt="image-20240714162940260" style="zoom:50%;" />

#### 如何计算Wasserstein

![image-20240714163036134](../assets/11.生成式对抗网络（GAN）/image-20240714163036134.png)

- y 如果是从 Pdata 来的,那我们要计算它的 D(y) 的**期望值，希望越大越好**

- y 如果是从 PG 来的,我们计算它的 D(y) 的**期望值**,但是**前面乘上一个负号，希望越小越好**

- D 必须要是一个足够平滑的 Function,**不可以是变动很剧烈的 Function**。从而，即使两个分布

  没有重叠时，也不会出现“过于剧烈”的D，保证收敛性。⇒避免了JS Divergence出现的问题。

  - 否则无法收敛（例如，置无限大）

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714163149144.png" alt="image-20240714163149144" style="zoom: 67%;" />

#### 如何实现训练

##### **原始——限制参数大小**

⇒并不一定能够让 Discriminator,变成 1-Lipschitz Function

![image-20240714164006622](../assets/11.生成式对抗网络（GAN）/image-20240714164006622.png)** ##### **Gradient Penalty**

https://arxiv.org/abs/1704.00028

![image-20240714164021774](../assets/11.生成式对抗网络（GAN）/image-20240714164021774.png)

在 Real Data 这边取一个 Sample,Fake Data 这边取一个 Sample,两点连线中间再取一个 Sample,我要求这个点它的 Gradient 要接近1

##### **Spectral Normalization（SNGAN）**

**保证让它是 1-Lipschitz Function。**

https://arxiv.org/abs/1802.05957

实作过程中，往往在之前iterate的结果之上继续进行训练

即使有WGAN，GAN还是很困难，为什么？

GAN training的困难之处

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714174014171.png" alt="image-20240714174014171" style="zoom:50%;" />

这两个 Network,这个 Generator 跟 Discriminator,它们是**互相砥砺,才能互相成长的**,只要其中一者,发生什麼问题停止训练,另外一者就会跟著停下训练,就会跟著变差。我们需要保证二者的loss在这一过程中不断下降。

**困难：**Discriminator 跟 Generator,它们互动的过程是自动的。我们不会在中间,每一次 Train Discriminator 的时候都换 Hyperparameter。如果有一次loss没有下降，那整个训练过程都有可能出现问题。

### 11.7.4 More tips

- [Tips from Soumith](https://github.com/soumith/ganhacks)

- [Tips in DCGAN: Guideline for network architecture design for image generation](https://arxiv.org/abs/1511.06434)

- [Improved techniques for training GANs](https://arxiv.org/abs/1606.03498)

- [Tips from BigGAN](https://arxiv.org/abs/1809.11096)

  

##  11.8 GAN for sequence Generation

### 11.8.1 特点-不能用gradient decent

****

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714174209835.png" alt="image-20240714174209835" style="zoom: 80%;" />

拿GAN生成文字是最困难的，因为decoder的变化并不会影响最终文字的结果，由于取了max，这一运算使得Discriminator的score对decoder参数不可微分，也就不能做GD。



这一过程就是：Decoder的参数有一点微小的变化，在后面进行Max概率处理后的到的token依旧是不变的，从而导致由discriminator输出的score也没有变化，因此不可微。

**思考：为什么在CNN中的Max pooling可以用GD?**

因为在做max pooling的时候我们知道元素导致了这个最大值，在做back propagation的时候只需要考虑这一个元素即可，其他元素置零。而对于GAN来说，我们是不知道哪个元素导致了“机器学习”这些字的输出的，中间存在一个gap.



**无法微分就可以用RL**(Reinforcement Learing)

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714174939249.png" alt="image-20240714174939249" style="zoom:67%;" />

RL和GAN都难train，所以很长时间都无法用GAN生成文字

### 11.8.2 ScrachGAN：Train Language GANs Form Scrach

直接从随机的初始化参数开始Train 它的 Generator,然后让 Generator 可以產生文字,它最关键的就是**爆调 Hyperparameter,跟一大堆的 Tips**

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714175132887.png" alt="image-20240714175132887" style="zoom:50%;" />

To learn More：其他的生成办法VAE、FLOW-based

## 11.9 有监督学习的生成模型

每一个图片都去配一个从 Gaussian Distribution Sample 出来的 Vector，Train 一个 Network,输入一个 Vector,输出就是它对应的图片,把对应的图片当做你训练的目标训练下去。

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714175403468.png" alt="image-20240714175403468" style="zoom:67%;" />

Generative Latent Optimization (GLO), https://arxiv.org/abs/1707.05776

Gradient Origin Networks, https://arxiv.org/abs/2007.02798

## 11.10 生成模型评估（Evaluation of Generation）

- 直觉的方法：人眼来看⇒不客观、不稳定、代价高
- 自动化方法：影像分类系统

### Quality：对一张图片

![image-20240714175510417](../assets/11.生成式对抗网络（GAN）/image-20240714175510417.png)

将图片输入影像辨识系统：

- 这个概率的分布如果越集中，说明产生的图片能够被影像分类系统“很肯定地”分辨出来，代表说现在產生的图片可能越好。
- 反之，如果产生的图像是“四不像”，图片分类系统感到困惑，概率分布变得平坦。

### Diversity：对所有（一批）图片

#### Diversity-Mode Collapse(模型崩溃）

⇒训练输出的分布局限在很小的范围。

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714175932030.png" alt="image-20240714175932030" style="zoom:50%;" />

#### Diversity-Mode Dropping(模型丢弃）

⇒训练输出的分布范围较大，但没有完全覆盖真实数据分布（多样性减小）。

![image-20240714180026288](../assets/11.生成式对抗网络（GAN）/image-20240714180026288.png)

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714180115615.png" alt="image-20240714180115615" style="zoom:50%;" />

### 评估多样性

把影像辨识系统对所有生成结果的输出平均起来

<img src="../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb9a35b8b-fe11-4aaa-9ac1-1d7808592205%2FUntitled.png" alt="img" style="zoom:50%;" />            <img src="../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fdebf4fb9-e29a-45ca-844f-1af672b08c05%2FUntitled.png" alt="img" style="zoom:50%;" />

- 平均分布“均匀”⇒多样性高
- 平均分布集中⇒多样性低（一种class占据较大部分）

### 量化指标

#### Inception Score（IS）

⇒基于CNN的Inception网络

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fac823af8-11ce-4e3f-ab93-1da73ae981e5%2FUntitled.png?table=block&id=87381f35-5a28-4407-9a54-a5c4b50f7ddf&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=1520&userId=&cache=v2" alt="img" style="zoom:33%;" />

将生成结果放进Inception网络，通过输出的分布结果来衡量。如果 Quality 高,那个 Diversity 又大,那 Inception Score 就会比较大。

#### Fréchet Inception Distance (FID)

<img src="../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F49ca3cb1-ac89-4a5e-8e82-1d698935305b%2FUntitled.png" alt="img" style="zoom: 33%;" />

一些情况下，生成的图像是“同一类别”的，看“分布”并不合适。

同样将图片送入Inception Network，取Softmax 之前的 Hidden Layer 输出的向量，来代表这张图片，利用这个向量来衡量两个分布之间的关系。

⇒假设真实数据和生成数据的两个分布，都是从高斯分布中抽样得到的，计算两个高斯分布之间的Fréchet Distance，越小代表分布越接近，图片品质越高。

**问题：**

- 将任意分布都视为“高斯分布”会有问题
- 计算FID需要大量采样，计算量大。



利用FID衡量不同GAN的性能

*Are GANs Created Equal,A Large Scale Study*

![img](../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F8770b2ce-cb55-4c52-80f8-5a3ba2c6beaf%2FUntitled.png)

- VAE 的方法显然是比较稳定的
- GAN可以產生远比 VAE 更好的结果
- 不知道是不是有某些 Network 架构,特别 Favor 某些种类的 GAN

### GAN记忆性

训练了一个 Generator,它產生出来的 Data,跟你的真实资料一模一样，没有意义。

![image-20240714191032256](../assets/11.生成式对抗网络（GAN）/image-20240714191032256.png)

或者只是学会了“左右反转”，也没有意义。

![img](https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F9fdbbbea-332a-42c1-a617-2d03580f4d8a%2FUntitled.png?table=block&id=4f34a7cc-15a2-47fd-aeee-e513a3549b9b&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=920&userId=&cache=v2)

### 其他的评估办法

https://arxiv.org/abs/1802.03446

![img](../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1e2e47bb-9200-40ee-8e2d-46e9838c618d%2FUntitled.png)

## 11.11 Conditional Generation

⇒supervised learning（需要标签）

#### Case 1:Text-to-image

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714191314971.png" alt="image-20240714191314971" style="zoom:50%;" />

![image-20240714191337535](../assets/11.生成式对抗网络（GAN）/image-20240714191337535.png)

**操控 Generator 的输出**,我们给它一个 Condition x，再从一个简单分布中抽样一个$z$,让generator**根据 x 跟 z 来產生 y。**

例如，x 就是一段文字，对希望得到的人脸形象进行描述。

##### 如何训练？

###### 魔改DIscriminator：

![image-20240714191448936](../assets/11.生成式对抗网络（GAN）/image-20240714191448936.png)

Discriminator 不是只吃图片 y,它还要吃 Condition x。一方面**图片要好**,另外一方面,这个**图片跟文字的叙述必须要是相配**的,Discriminator 才会给高分。

###### **准备资料**

![image-20240714191934311](../assets/11.生成式对抗网络（GAN）/image-20240714191934311.png)

- 需要“图片-文字”的成对资料⇒高分
- 需要准备“好图片-配错文字”的成对资料⇒低分

### Case 2：Image Translation（pix2pix）

#### pix2pix应用

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714191856463.png" alt="image-20240714191856463" style="zoom:50%;" />

- 给它房屋的设计图,然后让你的 Generator 直接把房屋產生出来
- 给它黑白的图片,然后让它把顏色著上
- 给它这个素描的图,让它把它变成实景 实物
- 那给它这个白天的图片,让它变成晚上的图片
- 有时候你会给它,比如说起雾的图片,让它变成没有雾的图片,把雾去掉

#### 利用Supervised Learning训练的问题→输出“模糊”

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714192022900.png" alt="image-20240714192022900" style="zoom: 67%;" />

直觉上，因為同样的输入可能对应到不一样的输出——同一个转角,那个小精灵可能左转,也可能右转,最后学到的就是同时左转跟右转

<img src="../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F082c6187-c5ea-4ba4-894b-37a34e0017c4%2FUntitled.png" alt="img" style="zoom:67%;" />

单纯用 GAN 的话,它有一个小问题,所以它產生出来的图片,比较真实,但是它的问题是它的创造力,想像力过度丰富,如果你要做到最好,往往就是 GAN 跟 Supervised Learning,同时使用。

### Case 3：声音-图片

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714192313225.png" alt="image-20240714192313225" style="zoom:50%;" />

那影片裡面有影像有画面,也有声音讯号，这一帧的画面,对应到这一小段声音。

### Case 4： 产生会动的人像

<img src="../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F0ff97c84-6a27-487b-82ed-35866f260209%2FUntitled.png" alt="img" style="zoom:40%;" />

## 11.12 Cycle GAN:Learning from Unpaired Data

⇒Unsupervised Learning

<img src="../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F803717c6-7ae1-4250-8e22-0af808285a73%2FUntitled.png" alt="img" style="zoom:33%;" />

我们有一堆X我们有一堆Y,但X跟Y是不成对的，就叫做unlabeled的资料。



### Case：影像风格转换

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714192728363.png" alt="image-20240714192728363" style="zoom:50%;" />

#### 问题归约：两个Domain之间的转换

<img src="../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff12ac49e-b9f1-4e24-a4a9-a2f4c8fb6c9b%2FUntitled.png" alt="img" style="zoom:50%;" />

#### 思路1：传统思维

- 套用原来的方法，将抽样的对象从“简单分布”改为$Domain\ x$；
- 对于Discriminater，利用$Domain\ y$中的图像训练，确保输出图片属于$Domain\ y$

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714192934498.png" alt="image-20240714192934498" style="zoom: 80%;" />

**问题：**

可能会忽略输入图像，仅把它当做一个高斯噪声⇒Unconditional GAN.

如何考虑“输入”的限制呢？

#### 思路2：Conditional GAN⇒没有成对资料

#### 思路3：Cycle GAN

有一个循环,从X到Y 在从Y回到X,它是一个cycle,所以叫做Cycle GAN。利用这种架构，强迫你的generator输出的Y domain的图片,跟输入的X domain的图片,有一些关係。

所以现在这边我们有**三个Network**

1. 第一个generator,它的工作是把X转成Y
2. 第二个generator,它的工作是要把Y还原回原来的X
3. 那这个discriminator,它的工作仍然是要看,蓝色的这个generator它的输出,像不像是Y domain的图

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714193204659.png" alt="image-20240714193204659" style="zoom:67%;" />

**训练**

两个generator：

- 第一个generator它的工作是,把X domain的图变成Y domain的图
- 第二个generator它的工作是,看到一张Y domain的图,把它**还原**回X domain的图

经过两次转换以后,输入跟输出要越接近越好。

实际上，Network其实非常懒惰,它输入一个图片,它往往就想输出很像的东西,它不太想做太复杂的转换⇒即使只是用默认的架构，也能实现这样的任务。

#### 双向Cycle GAN

<img src="../assets/11.生成式对抗网络（GAN）/image-20240714193511857.png" alt="image-20240714193511857" style="zoom: 67%;" />

- 把这个橙色的generator拿来,给它Y domain的图片,让它產生X domain的图片
- 然后在把蓝色的generator拿来,把X domain的图片,还原回原来Y domain的图片

## 11.13 More style-transfer GAN

与Cycle GAN类似：进行风格转换

<img src="../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb8c31333-fa4f-4255-bfa9-f69ef543d961%2FUntitled.png" alt="img" style="zoom:33%;" />

进阶版本：StarGAN——多种风格间转换

<img src="../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fbb63f098-e764-4597-a50c-1a9b8c516817%2FUntitled.png" alt="img" style="zoom:33%;" />

### Case ：Text Style Transfer

输入一个句子,输出另外一个风格的句子，主要利用Transformer的架构。

训练过程跟Cycle GAN是一模一样的,首先你要有训练资料,收集一大堆负面的句子,收集一大堆正面的句子。完全套用Cycle GAN的方法,完全没有任何不同。

<img src="../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F9fa7ba61-b499-4164-94f6-e8c0c9eaef27%2FUntitled.png" alt="img" style="zoom:33%;" />

假设我们是要负面的句子转正面的句子,discriminator要看说,现在generator的输出,像不像是真正的正面的句子⇒不可微分，需要**利用RL硬做。**

然后我们还要有另外一个generator,要有**两个generator**,这个generator要学会,把**正面的句子转回原来负面的句子。**

<img src="../assets/11.生成式对抗网络（GAN）/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F860cc279-0f66-49f2-ad8e-18712b11043e%2FUntitled.png" alt="img" style="zoom: 33%;" />

- 文本摘要
- 无监督翻译
- 无监督语音辨识
