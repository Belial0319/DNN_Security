# 14 BERT家族

## 14.1 技术前瞻

<img src="../assets/14.BERT家族/93a22f9a97694bdba4b4d6a75fc3f397.png" alt="img" style="zoom:33%;" />

在预训练模型上训练Bert，并在我们的数据上fine-tune所需要的模型！
就像学习英文一样！ 应该是通读英文文章后再去做题，而不是先做题再读懂文章！

<img src="../assets/14.BERT家族/image-20240720140659571.png" alt="image-20240720140659571" style="zoom:50%;" />

- ELMo
- Bert
- ERNIE
- Grover
- Bert&PALS

## 14.2 Pre-train Model

首先介绍预训练模型，预训练模型的作用是将一些token表示成一个vector

### Embedding

<img src="../assets/14.BERT家族/37b0ab3a41f0444e89b09c302fb335fb.png" alt="在这里插入图片描述" style="zoom: 33%;" />

比如：

- Word2vec
- Glove

但是对于英文，有太多的英文单词，这个时候应该对单个字符进行编码：

- FastText

<img src="../assets/14.BERT家族/354d1782ebd042bba44873039bacd43a.png" alt="在这里插入图片描述" style="zoom:33%;" />

对于中文，可以对部首偏旁，或者把中文字当做图片送人网络中得到输出：

<img src="https://img-blog.csdnimg.cn/7b68c57fb4dd4436a8946e8f5318de31.png" alt="在这里插入图片描述" style="zoom:33%;" />

上述方法的问题不会考虑每个句子意思中相同字会有不同的意思，产生相同的token：

<img src="https://img-blog.csdnimg.cn/b04d4c7e4b814a0990331007caf15f62.png" alt="在这里插入图片描述" style="zoom: 33%;" />

Contextualized word embedding

<img src="https://img-blog.csdnimg.cn/c5782008adab4850951ab3a9cf2c96ee.png" alt="在这里插入图片描述" style="zoom: 33%;" />

类似于seq2seq模型的encoder一样。

<img src="https://img-blog.csdnimg.cn/80681a4f676f4ba79b60bdbcf43490b2.png" alt="在这里插入图片描述" style="zoom: 33%;" />

但前面学到，只有在样本数据量比较大的情况下，模型训练会越精准。因此出现了两种方向,BERT要不做大，要不做小！ 大公司都是越来越大，但是穷人就是使得Bert越来越小！

- Bigger model 

![image-20240720142043444](../assets/14.BERT家族/image-20240720142043444.png)

不可能每个人都有足够资金来进行训练，因此也会出现小模型

- Smaller Model 

<img src="../assets/14.BERT家族/image-20240720142200510.png" alt="image-20240720142200510" style="zoom:50%;" />

其中最有名的是ALBERT，它神奇的地方在于基本都和BERT一样，不同的方法在于原来的BERT12层、24层都是不同的参数，但是ALBERT12层、24层都是一样的参数，但是效果比BERT还要好。

重点关注ALBERT，将模型变小的技术：

<img src="../assets/14.BERT家族/image-20240720142221952.png" alt="image-20240720142221952" style="zoom:50%;" />

### Network Architecture

除了让模型变小以外，现在在网络架构设计上也有突破：可以让模型读取较大的sequence token length

如果我们处理更多的句子，而不是仅仅的sequence的tokens，而是segment-level的！ 而是成段的，整个文章放入网络！

<img src="../assets/14.BERT家族/image-20240720142401890.png" alt="image-20240720142401890" style="zoom:50%;" />





## 14.3 How to fine-tune

具体的NLP任务来进行fine-tune！

<img src="../assets/14.BERT家族/image-20240720142508495.png" alt="image-20240720142508495" style="zoom:50%;" />

NLP的任务：

<img src="https://img-blog.csdnimg.cn/e3aae9e8d0394299bb79f403e25495c2.png" alt="img" style="zoom:50%;" />

**如果输入多个句子时**：

<img src="https://img-blog.csdnimg.cn/28a728f42b7b42858ae4b3493529e3bd.png" alt="img" style="zoom: 33%;" />

输入两个句子，中间有【SEP】作为隔绝！ 两个句子可以是查询和文件的差距，也可以是前提和假设的差异！

**如果是输出时**：

<img src="https://img-blog.csdnimg.cn/fbdd19db75934a1a94b63775baca133c.png" alt="img" style="zoom:33%;" />

**one class**：
我们可以使用CLS，使得其输出一个类！
或者我们取几个向量的平均！

<img src="https://img-blog.csdnimg.cn/8bf595010739435f8453bb9f9cfd05c0.png" alt="img" style="zoom:33%;" />

**class for each token**：
每个token输出一个类

<img src="https://img-blog.csdnimg.cn/21fe1b4682984340931a23a6a35f67ac.png" alt="img" style="zoom: 33%;" />

**copy from input**：
文件D和答案query一起作为输入放到QA model里，最终输出两个整型变量s和e，分别是在文中的答案

<img src="https://img-blog.csdnimg.cn/ec37316803fb44248e4703e11fa61882.png" alt="img" style="zoom:33%;" />

在BERT中如果操作的呢？ 我们只需要两个vector（没有懂怎么获得的，可能是预定义的），其中一个vector用来和document中的输出vector做dot product来，根据相似度来定是不是是不是起点； 而另一个vector是作为结尾的！

<img src="https://img-blog.csdnimg.cn/8aadd0f88edc441fada2f8c5e48e5508.png" alt="img" style="zoom:33%;" />

**General Sequence（v1）：**
Bert很适合Seq2Seq中的Encoder，然后经过Decoder来得到输出的句子！ 但是问题在于，现在的Task Specific需要的labeled data不应该多，而且Decoder最好是预训练的！ 但是现实是没有训练，那就会受到影响！

<img src="https://img-blog.csdnimg.cn/17c54d2b512240ee876aea7763f50fd8.png" alt="img" style="zoom:33%;" />

**General Sequence（v2）**：
我们可以以预测下一个token的方法来训练seq2seq！类似于Decoder，把得到的输出当作模型输入。

<img src="https://img-blog.csdnimg.cn/65da14c1dcea43bb9dcf2c129e480c1a.png" alt="img" style="zoom:33%;" />

第一种（V1）是将Pre-trained model固定住，第二种是一块训练（V2）！ 但是结果表明，往往一块训练效果会更好！

<img src="../assets/14.BERT家族/image-20240720143503053.png" alt="image-20240720143503053" style="zoom:50%;" />

**Adaptor**：
现在我们训练整个模型，如果一块训练的话，参数实在是太多了！

<img src="../assets/14.BERT家族/image-20240720143552222.png" alt="image-20240720143552222" style="zoom:33%;" />

那么我们可不可以选择其中一部分层进行训练呢？ 将这些层称为Adaptor层！其它地方固定！

<img src="../assets/14.BERT家族/image-20240720143646306.png" alt="image-20240720143646306" style="zoom:33%;" />

Adaptor的方法很多，而且插入到哪里都是需要研究的！ 我们以Transformer为例，我们在Feed-forward layer后加入Adaptor； 同时在训练之前我们不训练Adaptor，加入具体任务之后，我们才会训练Adaptor！ 右边的是Adaptor层的具体，确保参数不会太多！
![img](../assets/14.BERT家族/ba2d1dc0f5d94115bab856aa2fb378a1.png)

左侧0代表的是如果我们fine-tune整个model得到的结果，下图中蓝色的线表示的是训练倒数层，第一个点是倒数第一层，第二个点加入了倒数第二层，第三个点是加入了倒数第三层，以此类推! 而橙色的线就表示的是只训练其中的Adaptor！

<img src="../assets/14.BERT家族/image-20240720143934944.png" alt="image-20240720143934944" style="zoom: 50%;" />

Weighted Features：
其中$W_1$和$W_2$是可以被学出的！ 比如我们用最终的特征放入具体的任务中，那么这个参数就是可以被学习的！其中$W_1$和$W_2$对应于不同层产出的特征！ 不同层产出的特征是有不同的侧重点的，谁重要谁不重要是需要自己学的！
