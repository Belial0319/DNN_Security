# 2.深度学习基本思路

整体框架：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240614105649888.png" alt="image-20240614105649888" style="zoom:33%;" />

**Overfitting **过拟合就增大训练数据样本

## 2.1 局部最优和鞍点（凸优化）

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616115619053.png" alt="image-20240616115619053" style="zoom:33%;" />

对于**local minima**，很难解决

对于**sadle point**，利用泰勒多元微分展开

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616120037539.png" alt="image-20240616120037539" style="zoom:33%;" />

$H$为海塞矩阵，也就是二阶偏导

$L(\boldsymbol{\theta})\approx L(\boldsymbol{\theta}^{\prime})+\underbrace{(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime})^{T}g}+\underbrace{\frac{1}{2}(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime})^{T}H(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime})}$

此时处于鞍点，一阶导数为0，即第二项的$g$为0，因此只有$\frac12(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime})^TH(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime})$

通过判断这个的值，可以分析处于什么位置

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616120322562.png" alt="image-20240616120322562" style="zoom: 50%;" />

分析：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616120510571.png" alt="image-20240616120510571" style="zoom:33%;" />

此时就是判断$H$​的值就行了，正定？负定？

也可以判断二次型的特征值，判断特征值的正负情况

**举例**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616120941817.png" alt="image-20240616120941817" style="zoom:33%;" />

现在有一个极简的模型$y=w_1w_2x$，要使输入1，尽可能得到输出也为1

此时损失函数：$L=(\hat{y}-w_1w_2x)^2=(1-w_1w_2)^2$

此时的导数：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616121235105.png" alt="image-20240616121235105" style="zoom: 50%;" />

可以得到：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616121301907.png" alt="image-20240616121301907" style="zoom:50%;" />

此时可以判断是否在一个**saddel point**

判断在鞍点后，可以利用海塞矩阵进行进一步求解最优

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616121530950.png" alt="image-20240616121530950" style="zoom: 50%;" />

<img src="C:\Users\10056\Documents\WeChat Files\wxid_tkf0bkkvtr2522\FileStorage\Temp\9b638df26af8c6f6a060ea1fa713e58.jpg" alt="9b638df26af8c6f6a060ea1fa713e58" style="zoom: 10%;" />

例如：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616122912927.png" alt="image-20240616122912927" style="zoom: 50%;" />

下面会介绍一些超参数，和解决自动调整lr的算法

<img src="https://img-blog.csdnimg.cn/fed6c89c69e94be3bc821b3d3b187a4f.gif" alt="img" style="zoom: 50%;" />

## 2.2 Optimization

### 2.2.1 Batch

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616123904844.png" alt="image-20240616123904844" style="zoom:50%;" />

Full batch理论上运算时间长，但是每一次更新参数的功能比较好，Small Batch，每次只拿一个来更新，速度会快，但是跳的较大。

现在采用GPU进行并行计算，对于Large Batch的运算时间大大降低

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616124149411.png" alt="image-20240616124149411" style="zoom:33%;" />

对比：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616125027857.png" alt="image-20240616125027857" style="zoom: 33%;" />



****

### 2.2.2 Momentum

传统梯度下降（一般）

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616125503339.png" alt="image-20240616125503339" style="zoom:33%;" />

也就是加梯度反方向

现在：

***Gradient Descent + Momentum***

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616125713382.png" alt="image-20240616125713382" style="zoom: 50%;" />

相当于每次找最优解的时候都会考虑到前一次的方向，利用前一次的方向和负梯度方向得到新一次的方向

*有点像惯性*

$m^i$是相当于之前的梯度$g^0,g^1,...,g^{i-1}$的一个权重和

$m^0=0$

$m^1=-\eta g^{0}$
$$m^2=-\lambda\eta g^0-\eta g^1$$

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616130247175.png" alt="image-20240616130247175" style="zoom:50%;" />

相当于在最低点（局部最优后还要考虑到之前的运动方向，此时即使梯度为0.但也会继续运动）

### 2.2.3 Learning Rate

学习率就是学习步长，在梯度下降是确定方向，Learning rate 是确定在该方向所走步长。

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616130935549.png" alt="image-20240616130935549" style="zoom: 67%;" />

可以从右下角看到，即使步长较小，但是也不能达到预期的x值

最开始的梯度下降，此时学习率lr是固定的,也就是$\eta$固定

​                                                                  $\theta_i^{t+1}\leftarrow\theta_i^t-\eta\boldsymbol{g}_i^t\\g_i^t=\frac{\partial L}{\partial\theta_i}|_{\theta=\theta^t}$

此时会出现上面讲到的问题，对于固定的lr不一定能到达最终的理想点

**此时思考能不能把学习率lr在梯度变化较小的地方大一点，在梯度变化较大的地方少一点？**

也就是能不能把$\eta$随着时间变化而改变：

​                                                                  $\theta_i^{t+1}\leftarrow\theta_i^t-\frac\eta{\sigma_i^t}g_i^t$

如何设置$\frac{n}{\sigma_{i}^{t}}$?

#### 2.2.3.1 Root Mean Square

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616131936737.png" alt="image-20240616131936737" style="zoom:33%;" />

对于：

​                                                       $\sigma_i^t=\sqrt{\frac{1}{t+1}\sum_{i=0}^t(g_i^t)^2}$

当最开始斜率小（梯度小）的时候（平缓一点的地方）$\sigma_i^t$就会很小，因为相邻的两个梯度都很小（变化小），此时$\sigma_i^t$就大，所以：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616132708090.png" alt="image-20240616132708090" style="zoom: 50%;" />

可以看到蓝色曲线的斜率很小，变化也小，所以其步长就会大一点。

**Adagrad算法**

[【快速理解Adagrad】通俗解释Adagrad梯度下降算法-CSDN博客](https://blog.csdn.net/qq_45193872/article/details/124153859)

```python
def sgd_adagrad(parameters, sqrs, lr):
	eps = 1e-10
	for param, sqr in zip(parameters, sqrs):
 	sqr[:] = sqr + param.grad.data ** 2
 	div = lr / torch.sqrt(sqr + eps) * param.grad.data
 	param.data = param.data - div
```

缺点：对于特定的情形：**新月形**无法较好的修改学习率

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616170535024.png" alt="image-20240616170535024" style="zoom: 50%;" />

#### 2.2.3.2 RMSProp

引入一个超参数$\alpha$

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616171855676.png" alt="image-20240616171855676" style="zoom:50%;" />

利用超参数$\alpha$调控现在的$g_i^t$的重要性

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616172122578.png" alt="image-20240616172122578" style="zoom:50%;" />

对于最开始的梯度不大，此时可以调整$\alpha$使得$\sigma_{i}^{t-1}$较大，对于中间地方可以调整$\alpha$使得踩一脚刹车，此时梯度较大，可以通过变大$\alpha$​使得速度降下来。

```python
def rmsprop_update(parameters, gradients, sq_grads, lr=0.01, beta=0.9, epsilon=1e-8):
    for param, grad in zip(parameters, gradients):
        sq_grads[param] = beta * sq_grads[param] 
                          + (1 - beta) * (grad ** 2)
        param_update = lr / (np.sqrt(sq_grads[param]) + epsilon) * grad
        param -= param_update

```

在这个函数中，`parameters` 是模型参数列表，`gradients` 是对应的梯度列表，`sq_grads` 是历史梯度平方的累积（需要初始化），`lr` 是学习率，`beta` 和 `epsilon` 是 RMSProp 算法的超参数。

#### 2.2.3.3Adam

**Adam = RMSProp + Momentum**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616173709769.png" alt="image-20240616173709769" style="zoom:50%;" />

Pytorch已经有内置的Adam套件

现在利用Adam后可以得到：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616173920278.png" alt="image-20240616173920278" style="zoom:50%;" />

中间红圈里面的突然向上或向下突变，主要是由于中间横向移动的时候，梯度一直很小，导致$\sigma_{i}^{t-1}$

积累到很小的程度，然后步长就突变，同理达到很大后，又会回来。

### 2.2.4 Learning Rate Scheduling

之前我们一直用的方法：

​                                                                  $\theta_i^{t+1}\leftarrow\theta_i^t-\frac\eta{\sigma_i^t}g_i^t$

此时我们都是控制$\eta$为一个定值，当我们采用效果还可以的Adam修改后，发现在$\sigma_{i}^{t-1}$积累到很小的程度的时候，会发生突变，现在可以使得$\eta\rightarrow\eta^t$,让这个值随着时间变化而变化，此时可以得到：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616174633083.png" alt="image-20240616174633083" style="zoom:50%;" />

#### 2.2.4.1 Learing Rate Decay

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616174728101.png" alt="image-20240616174728101" style="zoom:33%;" />

强调随时间增长后单调递减

#### 2.2.4.2 Warm up

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616175413901.png" alt="image-20240616175413901" style="zoom:33%;" />

强调，先递增后递减，至于什么时候递增，什么时候递减要自行调配。目前主要用于训练***BERT***

### 2.2.5 Optimization 总结

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240616180348913.png" alt="image-20240616180348913" style="zoom:50%;" />
