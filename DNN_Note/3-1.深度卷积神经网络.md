# 6.深度卷积神经网络

## 6.1 LeNet



LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。
这个模型是由AT&T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像 :cite:`LeCun.Bottou.Bengio.ea.1998`中的手写数字。
当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果。

当时，LeNet取得了与支持向量机（support vector machines）性能相媲美的成果，成为监督学习的主流方法。
LeNet被广泛用于自动取款机（ATM）机中，帮助识别处理支票的数字。

总体来看，(**LeNet（LeNet-5）由两个部分组成：**)(~~卷积编码器和全连接层密集块~~)

* 卷积编码器：由两个卷积层组成;
* 全连接层密集块：由三个全连接层组成。

该架构如下所示。
![image-20240628162027773](C:\Users\10056\Desktop\科研\学习笔记\assets\3-1.深度卷积神经网络\image-20240628162027773.png)


每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均汇聚层。请注意，虽然ReLU和最大汇聚层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用$5\times 5$卷积核和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个$2\times2$池操作（步幅2）通过空间下采样将维数减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。

为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本。换言之，我们将这个四维输入转换成全连接层所期望的二维输入。这里的二维表示的第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示。LeNet的稠密块有三个全连接层，分别有120、84和10个输出。因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。





## 6.2 AlexNet

来源于[https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf]


在AlexNet网络问世之前，大量的学者在进行图像分类、分割、识别等工作时，主要是通过对图像进行特征提取或是特征+机器学习的方法。但是，这种手工准确提取特征是非常难的事情，而且即便使用机器学习的方法，整个算法的鲁棒性依然存在较大的问题。



因此，一直都有一种讨论：特征是不是也是可以进行学习的？如果是可以学习的，那么特征的表示是不是也存在层级问题（例如第一层为线或是点特征，第二层为线与点组成的初步特征，第三层为局部特征）？



从这一思想出发，特征可学习且自动组合并给出结果，这是典型的“end-to-end”，而特征学习与自由组合就是深度学习的黑盒子部分。



AlexNet出现后打破了原来众多学者的认知，它首次证明了学习到的特征可以超越手工设计的特征，从而越来越多的人开始重新审视深度学习算法并加入到研究的浪潮中。



### 6.2.1 AlexNet vs LeNet

通俗来讲，AlexNet是更深更大的LeNet，对于LeNet的主要改进就是：丢弃法、ReLu、MaxPooling

![../_images/alexnet.svg](https://zh-v2.d2l.ai/_images/alexnet.svg)

以上是从                                   **LeNet（左）**      和          **AlexNet（右）**             的区别

池化层：更大的池化窗口，使用最大池化层

卷积层：更大的核窗口和步长，因为图片更大了

### 6.2.2 AlexNet各层分析

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210708105304650.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1emhhbzk5MDE=,size_16,color_FFFFFF,t_70#pic_center)

 以上为单GPU情况，值得注意的一点：原图输入224 × 224，实际上进行了随机裁剪，实际对于所给数据集ImageNet中大小为227 × 227。

#### 6.2.2.1 卷积&池化层

**①卷积层C1**

C1的基本结构为：卷积–>ReLU–>池化

**卷积：**输入227 × 227 × 3，96个11×11×3的卷积核，不扩充边缘padding = 0，步长stride = 4，因此其FeatureMap大小为(227-11+0×2+4)/4 = 55，即55×55×96;

**激活函数：**ReLU；

**池化：**池化核大小3 × 3，不扩充边缘padding = 0，步长stride = 2，因此其FeatureMap输出大小为(55-3+0×2+2)/2=27, 即C1输出为27×27×96（此处未将输出分到两个GPU中，若按照论文将分成两组，每组为27×27×48）；



**②卷积层C2**

C2的基本结构为：卷积–>ReLU–>池化

**卷积：**输入27×27×96，256个5×5×96的卷积核，扩充边缘padding = 2， 步长stride = 1，因此其FeatureMap大小为(27-5+2×2+1)/1 = 27，即27×27×256;

**激活函数：**ReLU；

**池化：**池化核大小3 × 3，不扩充边缘padding = 0，步长stride = 2，因此其FeatureMap输出大小为(27-3+0+2)/2=13, 即C2输出为13×13×256（此处未将输出分到两个GPU中，若按照论文将分成两组，每组为13×13×128）；



 **③卷积层C3**

 C3的基本结构为：卷积–>ReLU。**注意一点：此层没有进行MaxPooling操作。**

**卷积：**输入13×13×256，384个3×3×256的卷积核， 扩充边缘padding = 1，步长stride = 1，因此其FeatureMap大小为(13-3+1×2+1)/1 = 13，即13×13×384;

**激活函数：**ReLU，即C3输出为13×13×384（此处未将输出分到两个GPU中，若按照论文将分成两组，每组为13×13×192）；



**④卷积层C4**

C4的基本结构为：卷积–>ReLU。**注意一点：此层也没有进行MaxPooling操作。**

**卷积：**输入13×13×384，384个3×3×384的卷积核， 扩充边缘padding = 1，步长stride = 1，因此其FeatureMap大小为(13-3+1×2+1)/1 = 13，即13×13×384;

**激活函数：**ReLU，即C4输出为13×13×384（此处未将输出分到两个GPU中，若按照论文将分成两组，每组为13×13×192）；



⑤**卷积层C5**

C5的基本结构为：卷积–>ReLU–>池化

**卷积：**输入13×13×384，256个3×3×384的卷积核，扩充边缘padding = 1，步长stride = 1，因此其FeatureMap大小为(13-3+1×2+1)/1 = 13，即13×13×256;
**激活函数：**ReLU；

**池化：**池化核大小3 × 3， 扩充边缘padding = 0，步长stride = 2，因此其FeatureMap输出大小为(13-3+0×2+2)/2=6, 即C5输出为6×6×256（此处未将输出分到两个GPU中，若按照论文将分成两组，每组为6×6×128）；



#### 6.2.2.2 连接层

⑥**全连接层FC6**

FC6的基本结构为：全连接–>>ReLU–>Dropout

**全连接：** 此层的全连接实际上是**通过卷积进行的**，输入6×6×256，4096个6×6×256的卷积核，扩充边缘padding = 0, 步长stride = 1, 因此其FeatureMap大小为(6-6+0×2+1)/1 = 1，即1×1×4096;
**激活函数：**ReLU；
**Dropout：**全连接层中去掉了一些神经节点，达到防止过拟合，FC6输出为1×1×4096；



**⑦ 全连接层FC7**

FC7的基本结构为：全连接–>>ReLU–>Dropout

**全连接：**此层的全连接，输入1×1×4096;

**激活函数：**ReLU；

**Dropout：**全连接层中去掉了一些神经节点，达到防止过拟合，**FC7**输出为1×1×4096；



⑧**连接层FC8**

FC8的基本结构为：全连接–>>softmax

**全连接：**此层的全连接，输入1×1×4096;

**softmax：**softmax为1000，**FC8**输出为1×1×1000；



### 6.2.3 AlexNet网络结构的主要贡献

#### 6.2.3.1 ReLU激活函数的引入

    采用修正线性单元(ReLU)的深度卷积神经网络训练时间比等价的tanh单元要快几倍。而时间开销是进行模型训练过程中很重要的考量因素之一。同时，ReLU有效防止了过拟合现象的出现。由于ReLU激活函数的高效性与实用性，使得它在深度学习框架中占有重要地位。

#### 6.2.3.2 层叠池化操作

    以往池化的大小PoolingSize与步长stride一般是相等的，例如：图像大小为256*256，PoolingSize=2×2，stride=2，这样可以使图像或是FeatureMap大小缩小一倍变为128，此时池化过程没有发生层叠。但是AlexNet采用了层叠池化操作，即PoolingSize > stride。这种操作非常像卷积操作，可以使相邻像素间产生信息交互和保留必要的联系。论文中也证明，此操作可以有效防止过拟合的发生。

#### 6.2.3.3 Dropout操作

    Dropout操作会将概率小于0.5的每个隐层神经元的输出设为0，即去掉了一些神经节点，达到防止过拟合。那些“失活的”神经元不再进行前向传播并且不参与反向传播。这个技术减少了复杂的神经元之间的相互影响。在论文中，也验证了此方法的有效性。

#### 6.2.3.4 网络层数的增加

    与原始的LeNet相比，AlexNet网络结构更深，LeNet为5层，AlexNet为8层。在随后的神经网络发展过程中，AlexNet逐渐让研究人员认识到网络深度对性能的巨大影响。当然，这种思考的重要节点出现在VGG网络（下文中将会讲到），但是很显然从AlexNet为起点就已经开始了这项工作。


### 6.2.4 AlexNet代码实现



```python
import time
import torch
from torch import nn, optim
import torchvision
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class AlexNet(nn.Module):
    def __init__(self):
        super(AlexNet, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding
            nn.ReLU(),
            nn.MaxPool2d(3, 2), # kernel_size, stride
            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数
            nn.Conv2d(96, 256, 5, 1, 2),
            nn.ReLU(),
            nn.MaxPool2d(3, 2),
            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。
            # 前两个卷积层后不使用池化层来减小输入的高和宽
            nn.Conv2d(256, 384, 3, 1, 1),
            nn.ReLU(),
            nn.Conv2d(384, 384, 3, 1, 1),
            nn.ReLU(),
            nn.Conv2d(384, 256, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(3, 2)
        )
         # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合
        self.fc = nn.Sequential(
            nn.Linear(256*5*5, 4096),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(),
            nn.Dropout(0.5),
            # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000
            nn.Linear(4096, 10),
        )

def forward(self, img):
    feature = self.conv(img)
    output = self.fc(feature.view(img.shape[0], -1))
    return output
```

## 6.3 VGG