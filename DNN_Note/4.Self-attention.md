# 4 Self-attention

## 4.1 使用场景

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620074759715.png" alt="image-20240620074759715" style="zoom:33%;" />

①对于输入是一组向量，而且长度不统一，这里适合使用self-attention

**这就引入一个问题，如何对文本向量化？**

在4.2中会详细讲解Text Embedding

②语音输入

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620075309160.png" alt="image-20240620075309160" style="zoom:33%;" />

③Social Network

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620075359037.png" alt="image-20240620075359037" style="zoom:50%;" />

一个人的信息也可以看作一个向量

## 4.2 Text Embedding

**文本向量化（Text Embedding）**：将文本数据（词、句子、文档）表示成向量的方法。

**词向量化**将词转为二进制或高维实数向量，**句子和文档向量化**则将句子或文档转为数值向量，通过平均、神经网络或主题模型实现。

### 4.2.1 词向量化

词向量化就是将单个词转化为数值向量。主要包括以下两种：

#### 4.2.1.1 主要类别

#### ①**独热编码 One-hot Encoding**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620074914852.png" alt="image-20240620074914852" style="zoom:33%;" />

为每个词分配一个唯一的二进制向量，其中只有一个位置是1，其余位置是0。但此时无法观察两个品类中的相互关系。

#### ②**词嵌入 Word Embedding**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620075034059.png" alt="image-20240620075034059" style="zoom:50%;" />

如Word2Vec, GloVe, FastText等，将每个词映射到一个高维实数向量，这些向量在语义上是相关的。另外比如：

<img src="https://img-blog.csdnimg.cn/img_convert/972ad39cd35e1401f5c67a73fc647164.jpeg" alt="img" style="zoom:50%;" />

#### 4.2.1.2 Word2Vec原理



### 4.2.2 句子向量化

句子向量化也就是将整个句子转换为一个数值向量。

包括的常规方法有：

简单平均/加权平均：对句子中的词向量进行平均或根据词频进行加权平均。

递归神经网络（RNN）：通过递归地处理句子中的每个词来生成句子表示。

卷积神经网络（CNN）：使用卷积层来捕捉句子中的局部特征，然后生成句子表示。

自注意力机制（如Transformer）：如BERT模型，通过对句子中的每个词进行自注意力计算来生成句子表示。













## 4.2 输出形式

### 4.2.1 固定输出

一组向量，比如说有n个向量，输出n个对象，对象可以是预测值，也可以是分类的class

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620075710146.png" alt="image-20240620075710146" style="zoom:50%;" />

例如可以对一段话进行语义分析

**POS tagging**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620075750754.png" alt="image-20240620075750754" style="zoom:50%;" />

**HW2**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620075811409.png" alt="image-20240620075811409" style="zoom:50%;" />

**Social Network 推荐**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620080011170.png" alt="image-20240620080011170" style="zoom:50%;" />

### 4.2.2 单输出

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620075924928.png" alt="image-20240620075924928" style="zoom:50%;" />

可以用于对句子进行情感分析，是正面还是负面，以及对一段语音进行识别是谁在说话，或者对一个高分子进行结构识别

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620080140213.png" alt="image-20240620080140213" style="zoom:50%;" />

### 4.2.3 不定输出（seq2seq)

此时输出的结果，由机器自己决定输出多少

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620080313787.png" alt="image-20240620080313787" style="zoom:50%;" />

## 4.3 Sequence Labeling

属于固定输出类型。输入多少，输出多少

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620080655876.png" alt="image-20240620080655876" style="zoom:33%;" />

这里的两个saw要输出不一样的东西，如果仅仅是直接丢到FC里面，输出必然是相同的。现在需要做的就是联系上下文。也就是把前后几个内容都穿起来。

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620080835326.png" alt="image-20240620080835326" style="zoom:50%;" />

此时也就是可以设置一个window，这个window里面的数据相当于邻居，一个向量再进行FC时要考虑到同window下的其他邻居。

**但是window的大小如何设计？**

假如说现在要分析整段的很长的句子，要是把window设置很大，会造成资源浪费，算力也不支持。也容易overfitting

## 4.4 Self-attention工作过程

具体实行方法:

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620081234633.png" alt="image-20240620081234633" style="zoom:50%;" />

其中self-attentionh会对每一个输入进行计，并各自输出一个包含其他信息的新的数据块，此时输出的每一块都有其他块的信息。同时self-attention也可以一直交替使用。

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620081429094.png" alt="image-20240620081429094" style="zoom: 50%;" />

《Attention is all you need》

https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf

运行过程

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620081853182.png" alt="image-20240620081853182" style="zoom:33%;" />

对于每一个输出都要考虑其他的全部元素

### Step 1 分析输入的相关性

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620082048925.png" alt="image-20240620082048925" style="zoom:50%;" />

如何获得$a^1$和$a^4$的相关性?

#### ① Dot-product

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620082248737.png" alt="image-20240620082248737" style="zoom: 50%;" />

​                                                             $q=W^q*a^1$           $k=W^k*a^4$       

​                                                                              $\alpha=q*k$

####  ② Additive

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620082600899.png" alt="image-20240620082600899" style="zoom:33%;" />

### Step 2 计算关联性

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620082746198.png" alt="image-20240620082746198" style="zoom:50%;" />

计算完关联性后（也要计算自己和自己的关联性）

**注意此时计算$q^1$利用一个$W^q$，而计算其他的$k^i$都是利用一个矩阵$W^k$**

**也就是说至始至终只用了两个矩阵$W^q,W^k$**

计算完再进入一个softmax（也可用ReLU)

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620082900967.png" alt="image-20240620082900967" style="zoom:50%;" />

### Step 3 结果计算

现在我们根据$\alpha_{1,i}^{\prime}$可以得到与$\alpha^1$的相关数，现在对分数进行重要信息提取

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620083630659.png" alt="image-20240620083630659" style="zoom:50%;" />

此时利用一个新的矩阵$W^v$，分别与原始的$a^i(i=1,2,3,4)$相乘，得到$v^i(i=1,2,3,4)$​

然后把$v^i*\alpha_{1,i}^{\prime},=$再相加得到$b^1$

​                                                                $b^{1}=\sum_i\alpha_{1,i}^{\prime}v^i$

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620084019880.png" alt="image-20240620084019880" style="zoom:50%;" />

注意此时的$b^1,b^2,b^3,b^4$​是同时被计算出来的

### Step 4 总体分析

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620084505147.png" alt="image-20240620084505147" style="zoom:50%;" />

每一个输入都会有各自对应的$q,k,v$矩阵

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620084539122.png" alt="image-20240620084539122" style="zoom: 50%;" />

整体可以看作三个大的矩阵乘法

​                                                                             $Q=W^q \cdot I$            $K=W^k \cdot I$             $ V=W^v \cdot I$​

​               											$I=[a^1,a^2,a^3,a^4]$

***注，这里的$W^q,W^k.W^v$是系统公用的。（参数共享）***

**attention score**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620084817364.png" alt="image-20240620084817364" style="zoom:50%;" />

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620085004782.png" alt="image-20240620085004782" style="zoom: 50%;" />

这里相当于得到attention score矩阵$A$：

​                                                                             $A=K^T \cdot Q$

再通过一轮softmax得到Attention Matrix$A^{\prime}$

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620085548530.png" alt="image-20240620085548530" style="zoom:33%;" />

​                                                                      $A^{\prime}=A=K^T \cdot Q$

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620085830758.png" alt="image-20240620085830758" style="zoom:50%;" />

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620085950935.png" alt="image-20240620085950935" style="zoom: 50%;" />

现在只有$W^q,W^k.W^v$需要计算



## 4.5 Multi-head Self-attention

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620090623647.png" alt="image-20240620090623647" style="zoom:50%;" />

也就是对原来的$q^i,k^i,v^i$​进行分解

这样做的好处是:

1. 可以让模型在不同的子空间中学习到不同的相关性,从而提高特征提取能力。
2. 相比单一的注意力机制,多头注意力能够更好地建模复杂的依赖关系。
3. 相比简单的全连接层,多头注意力能够更有效地捕获输入之间的关联。



## 4.6 Positional Encoding

我们再回顾一下最开始的

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620080655876.png" alt="image-20240620080655876" style="zoom:33%;" />

注意此时的单词是有**顺序**的，而上面我们利用self attention的一系列操作都是通过矩阵运算得到，而矩阵运算的行序改变并不影响，即把$a^2$和$a^4$​位置对换不会有什么影响。这时就要使用Positional Encoding。 

​                                  <img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620101915445.png" alt="image-20240620101915445" style="zoom:50%;" />

 

也就是在$a^i$旁边加上一个$e^i$,其中$e^i$​可以由网络计算出来

## 4.7 Self-attention 应用

### 4.7.1 **NLP**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620102233165.png" alt="image-20240620102233165" style="zoom:50%;" />

### 4.7.2 **语音相关**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620102443684.png" alt="image-20240620102443684" style="zoom:50%;" />

**Truncated Self-attention**

### 4.7.3 **图像相关**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620102626534.png" alt="image-20240620102626534" style="zoom:50%;" />

这里相当于把每个通道相同位置的元素提取出来作为一个向量（inputs）,从而可以利用self-attention

## 4.8 模型对比

### 4.8.1 Self-attention v.s. CNN

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620102939241.png" alt="image-20240620102939241" style="zoom: 50%;" />

这里的Self-attention相当于是看了整张图片，分析里面的关联性，而CNN是对每一个receptive field配置一组filter,分析这个field里面的数据，也就是说CNN是低配版Self-attention

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620103226652.png" alt="image-20240620103226652" style="zoom:50%;" />

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620103517768.png" alt="image-20240620103517768" style="zoom:50%;" />

### 4.8.2 Self-attention v.s. RNN

RNN是一类专门用于处理序列数据的神经网络模型。它与前馈神经网络的主要区别在于,RNN在处理序列数据时会保留之前的隐藏状态,从而能够利用之前的信息来处理当前的输入。

即把上一个输出当作下一个的输入，从而使得最终输出的结果包含前面所有的信息。RNN只考虑前一个的输入



<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620103652207.png" alt="image-20240620103652207" style="zoom: 50%;" />



最大的问题不能并行计算，智能串行

之前有说到，self-attention的参数可以同时产生

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620104009389.png" alt="image-20240620104009389" style="zoom:50%;" />

## 4.9 Self-attention for Graph

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620104355184.png" alt="image-20240620104355184" style="zoom:50%;" />

对于Graph，由于有明确的关联关系，如右图的邻接矩阵，此时我们在计算 attention score 时只用计算有关联边的节点即可

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620104530211.png" alt="image-20240620104530211" style="zoom:50%;" />

对于没有关联的点，相互间的score记为0即可。这就是一种类型的GNN

