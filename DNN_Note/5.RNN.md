# 5 RNN（Recurrent Neural Network）

## 5.1 RNN基本原理

### 5.1.1 基本过程

课程引入：

**Slot Filling**

比如说现在对于一个语音助手，你和他说：帮我查询一下arrive **Taipei **on **November $2^{nd}$**的机票。

这个时候语音助手就会打开一个系统搜寻信息。如何进行搜寻呢？

![image-20240620131440337](C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620131440337.png)

这个时 

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620131956583.png" alt="image-20240620131956583" style="zoom:50%;" />

对于输入的Taipei(已经转化为序列)可以通过classification判断在哪一个类别，但是如何确定是在destination里面？

比如说例如上图，leave Taipei on Novermber 这时的Taipei并不是Destination，如何进行？？

引入记忆模块，先看过arrive 再看Taipei，就可以确定。

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620132746763.png" alt="image-20240620132746763" style="zoom:50%;" />

类似如此，在第一层输出后，将结果复制到中，过程如下：

例如现在有一定输入:

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620132912485.png" alt="image-20240620132912485" style="zoom:33%;" />

现在需要对$a_1,a_2$赋初始值，假设都为0.

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620133220008.png" alt="image-20240620133220008" style="zoom:33%;" />

为简化运算，设置整个函数为线性函数，且权重都为1，bias为0

此时通过计算可以得到：注意此时：

**绿色块 = 蓝色块与黄色块的线性组合**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620133541209.png" alt="image-20240620133541209" style="zoom:50%;" />

得到【2，2】后，继续计算进一步的网络，得到【4，4】

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620133307463.png" alt="image-20240620133307463" style="zoom:50%;" />

此时再将绿色的数据保存到蓝色块中，再进行运算，此时蓝色块的值改变，再利用

绿色块 = 蓝色块与黄色块的线性组合，继续获得下一个参数

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620133409311.png" alt="image-20240620133409311" style="zoom: 33%;" />



总体：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620133946804.png" alt="image-20240620133946804" style="zoom:50%;" />



注意此时是一个Network 只是在不同时间点被使用。

### 5.1.2  训练过程

#### 5.1.2.1 损失函数设计

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240623104415544.png" alt="image-20240623104415544" style="zoom:50%;" />

类似一个多分类问题，利用每一时刻，每一个单词对应的slot的编码数值与所输出的$y^i$进行计算，得到损失函数。

这里和softmax一样，采用交叉熵来进行设计损失函数



#### 5.1.2.2 学习方法

也可以和常规神经网络一样使用Gradient Decent方法学习参数，由于此时也会将时间作为参数，因此可以采用一直新的方法BPTT[[BPTT算法详解：深入探究循环神经网络（RNN）中的梯度计算【原理理解】_rnn bptt-CSDN博客](https://blog.csdn.net/qq_22841387/article/details/139283146)]

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240623104840362.png" alt="image-20240623104840362" style="zoom:33%;" />

为什么在这里不适用梯度下降？
在传统的反向传播算法中，处理静态数据时，网络的输出 $$\hat{y} $$通常只依赖于当前时刻的隐藏状态 $$h $$​，其更新规则可以表示为：

​                                                                     $\begin{aligned}h&=Wx+b\\\hat{y}&=Vh+c\end{aligned}$

其中，$$h$$是隐藏状态，$x$是输入，$W$和 $V$是网络的参数，$b$和 $c$​是偏置项。

<img src="https://img-blog.csdnimg.cn/direct/bec4bb1ffe464c54b17848613b997a1f.png" alt="RNN结构" style="zoom: 33%;" />

常见的RNN结构如上，在RNN中，参数$U$、$V$和$W$是共享的，这意味着它们在每个时间步都保持不变。这意味着它们的值在整个模型运行过程中 **始终保持一致** 。



与传统反向传播算法不同，BPTT（Back-Propagation Through Time）算法引入了时间维度，并考虑了序列数据中的时序关系。在 BPTT 中，隐藏状态$\hat{h} $的更新规则包含了当前时刻的输入$X_t$

  和上一个时刻的隐藏状态$h_{t-1}$ ，从而能够更好地捕捉到序列数据中的时间相关性。
                                                              $\begin{aligned}h_t&=f(UX_t+Wh_{t-1})\\\hat{y}_t&=f(Vh_t)\end{aligned}$

#### 5.1.2.3 结果分析

对于基于RNN的网络不是很容易去学习，因为它的Loss不会慢慢下降并趋近收敛。而是可能会出现很大的波动，

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240623110129814.png" alt="image-20240623110129814" style="zoom: 50%;" />

蓝色的是ideal,绿色的是实际的了。

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240623110318763.png" alt="image-20240623110318763" style="zoom:33%;" />

对于RNN的Error surface它的跳动会很大，什么东西造成这种现象？

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240623104840362.png" alt="image-20240623104840362" style="zoom:33%;" />

对于上面这个图，我们需要学习$W$，根据之前的例子，我们现在假设一个极简的RNN模型：
![image-20240623111036046](C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240623111036046.png)

此时假设输入的向量为[1,0,0……，0]，neuron采用线性函数，参数为0，bias都为0.

可以看到$y^{1000}$输出的结果为$W^{1000}$,此时$y^{1000}$的值会极大程度被$W$影响，例如：

$W=1$                    $y^{1000}=1$​

$W=1.01$               $y^{1000}\approx20000$

$W=0.99$                $y^{1000}\approx0$

$W=1.01$                $y^{1000}\approx0$​

可以看到$W$​在[0,1]内变化不大，也就是上面图中的平坦部分，而在1附近，数值轻微变化会导致剧烈的变动

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240623111746503.png" alt="image-20240623111746503" style="zoom:33%;" />

## 5.2 RNN模型相关变体

### 5.2.1 Elman Network & Jordan Network

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620134235349.png" alt="image-20240620134235349" style="zoom: 67%;" />

Jordan Network 就是把一个的输出，作为下一个计算的记忆单元

在实际应用中,Elman Network和Jordan Network的特点可以被用来解决不同类型的序列建模问题:

1. Elman Network:
   - 适用于需要建模序列数据中上下文依赖关系的任务,如**语言模型、机器翻译**等。
   - 例如,在语言建模中,Elman Network可以更好地学习单词之间的上下文联系,从而生成更加连贯、语义更丰富的文本。
2. Jordan Network:
   - 适用于需要利用之前输出信息来生成当前输出的任务,如**语音合成、文本生成**等。
   - 例如,在语音合成中,Jordan Network可以更好地利用前一时刻合成的语音信息,从而生成更加连贯、自然的语音输出。

### 5.2.2 Bi-RNN（Bidirectional RNN）

也就是RNN可以双向同时进行运算

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620134405252.png" alt="image-20240620134405252" style="zoom:50%;" />

效果就是看到的范围比较广

Bi-RNN相比基本RNN的优点包括:

1. 利用更丰富的上下文信息: Bi-RNN同时考虑了序列的正向和反向信息,能够更好地捕捉序列中的上下文依赖关系。
2. 更好的梯度流动: Bi-RNN能够缓解基本RNN中的梯度消失/爆炸问题,使训练更加稳定。

Bi-RNN在以下场景中广泛应用:

- 自然语言处理: 如文本分类、命名实体识别、机器翻译等。

- 语音识别: 利用音频序列的正向和反向信息进行更准确的语音识别。

- 生物信息学: 如DNA序列分析、蛋白质结构预测等生物序列建模任务

  

## 5.3 RNN存在的问题



在5.1.2.3 中分析了RNN训练过程中的一些问题，以下对其原因进行分析。

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620212254947.png" alt="image-20240620212254947" style="zoom:50%;" />

以上是传统的RNN。这里要回顾一下关于**梯度消失（Gradient Vanishing)**的问题[[深度学习中梯度消失和梯度爆炸的根本原因及其缓解方法_神经网络_jiangtao129-GitCode 开源社区 (csdn.net)](https://gitcode.csdn.net/662a04569ab37021bfb12f6a.html?dp_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpZCI6NDU4NTY2NiwiZXhwIjoxNzE5NDk0OTUxLCJpYXQiOjE3MTg4OTAxNTEsInVzZXJuYW1lIjoiaHVhd2Vpb2QifQ.36q1r2ZkEj7olVYVPO-zYFCDi7SoWgQMDn25GWcePPg#devmenu1)]**误差梯度（gradient）被反向传播回网络的输入层时，梯度的值变得非常小，甚至接近于零，导致网络权重的更新非常缓慢或者几乎不更新**。



梯度消失通常发生在**使用传统的激活函数（如Sigmoid或Tanh）的深度网络中。这些激活函数的导数在输入值较大或较小时都非常接近于零**。

在之前的神经网络中已经学习过，在循环神经网络中，我们可以将其中的每个时间步骤视为一个层。为了训练一个递归神经网络，我们使用一种称为通过时间反向传播的方法，梯度值在每个时间步长传播时将呈指数级收缩。梯度值将用于在神经网络权重中进行调整，从而允许其学习，小的渐变意味着小的调整，这将导致最前面的层没有优化。

而这种梯度消失，就会引起短期记忆问题，由梯度消失引起的短期记忆问题是RNN的痛点，RNN的特点本来就是能“追根溯源“利用历史数据，但由于梯度消失，RNN不会跨时间步骤学习远程依赖性。

对于解决梯度消失可以采用更换激活函数，**使用ReLU及其变种**、**改进网络传播架构**等方法。下面就是解决该问题引入的LSTM



## 5.4 LSTM(Long Short-term Memory)

如上文所述，长短期记忆（Long short-term memory, LSTM）是对RNN结构的一种改进，主要是为了解决长序列训练过程中的梯度消失问题，让循环神经网络具备更强更好的记忆性能。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。

### 5.4.1 LSTM基本架构

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620135050645.png" alt="image-20240620135050645" style="zoom:50%;" />

Input Gate、Output Gate 、Forget Gate这三个门会在一定条件下打开或关闭，其中打开关闭的时机也是机器自己学习得到的，

比如当Input Gate 关闭时，无法输入，其他同理。

对于Forget Gate：打开的时候被遗忘，不打开的时候被记住

图中可以看到有四个输入，一个输出。

具体如下：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620135502152.png" alt="image-20240620135502152" style="zoom:50%;" />

其中的function一般采用sigmoid,这是为了使输出值落到[0,1]，从而方便进行判断是否打开Gate。

①对于输入$z$,通过function $g$可以得到$g(z)$

②对于输入$z_i$,通过function $f$可以得到$f(z_i)$​

③然后对$g(z)$和$f(z_i)$进行相乘，得到$g(z)f(z_i)$

④对于输入$z_f$,通过function $f$可以得到$f(z_f)$,

⑤中心Cell处会存在一个常数$c$,与④中相乘得到$cf(z_f)$

⑥计算$c^{\prime}=g(z)f(z_i)+cf(z_f)$​，并将Cell（Memory）中的$c$更新为$c^{\prime}$

⑦$c^{\prime}$经过function $h$得到$h(c^{\prime})$

⑧对于输入$z_o$,通过function $f$可以得到$f(z_o)$

⑨如果$f(z_o)==1$，则将$h(c^{\prime})$与$f(z_o)$相乘得到结果：$a=h(c^{\prime})f(z_o)$。并输出$a$​

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620140945243.png" alt="image-20240620140945243" style="zoom: 50%;" />

### 5.4.2 LSTM举例

LSTM例子：

![image-20240620200705592](C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620200705592.png)

假设$x_2$表示Forget Gate、$x_3$表示Output Gate

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620200828684.png" alt="image-20240620200828684" style="zoom:33%;" />

如果$x_2=1$，则把$x_1$放入Memory进行累加，若$x_2=-1$，则reset memory

如果$x_3=1$，则输出memory的数字

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620201848362.png" alt="image-20240620201848362" style="zoom: 40%;" />

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620201947781.png" alt="image-20240620201947781" style="zoom:50%;" />

注意这里对于$x_2=1$，则把$x_1$放入Memory进行累加，若$x_2=-1$​，则reset memory

可以观察Input Gate，此时设置bias为-10，相当于平常就是被关闭的，因为若$x_2$不为1.则线性累加到肯定是负数，通过一个sigmoid，可以得到Input Gate的值为0，即为关闭状态。

同理Forget Gate的bias为10，一般就是打开状态，平常都会保存记忆。

Output Gate的bias为-10，平常就是被关闭的。

现在来进行输入：

**①第一个输入[3,1,0]**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620214754124.png" alt="image-20240620214754124" style="zoom:50%;" />

此时主要看Input Gate计算得到$3*0+1*100+0*0+1*(-10)=90$经过sigmoid后约等于1；后面按照5.5.1内容进行计算，计算Output gate可以得到近似为0，因此不输出。



**②第二个输入[4,1,0]**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620215219479.png" alt="image-20240620215219479" style="zoom:50%;" />

同上，仍然关闭。但是会把$x_1$的4累加到Cell中，也就是7

**③第三个输入[2,0,0]**

同上计算Output Gate 仍然关闭

**④第四个输入[1,0,1]**

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620215407860.png" alt="image-20240620215407860" style="zoom:50%;" />

由于$x_2=0,x_3=1$此时input gate关闭，也就是$x_1$​不进入计算，通过图片也可以看到，对于Cell下的区块，他们的乘积为0.因此只考虑Forget Gate,由于此时常开。

7而对于Output Gate，由于$x_3=1$​因此可以输出，最后把记忆单元Cell中的7输出。

这框架好6

### 5.4.3 LSTM模型理解

#### 5.4.3.1 LSTM and Original Network

这是传统的，两个参数输入即可

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620220619150.png" alt="image-20240620220619150" style="zoom:50%;" />

在进行LSTM时，上面学到会有四个输入，1个输出，画图如下：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620220719262.png" alt="image-20240620220719262" style="zoom:50%;" />

这个时候输入也是两个参数，只是多乘了几个矩阵，对应各自的输入口。每一个LSTM都可以看作一个neuron，这就返回到了最开始的NN模型

#### 5.4.3.2 LSTM and RNN

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620221615039.png" alt="image-20240620221615039" style="zoom:50%;" />

①获得最下面的输入1：

对于$x^t$进行一个Transfer的变换后得到一个矩阵$z$，其中$z$的每一维就是最下面的每一个LSTM框架输入

②获得Input Gate的输入2：

同理得到$z^i$,，其中$z^i$​的每一维就是左下角的每一个LSTM框架Input Gate输入

③…………④依次可以得到四个矩阵：$z^f,z^i,z,z^o$

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620222318481.png" alt="image-20240620222318481" style="zoom: 50%;" />

此时四个矩阵$z^f,z^i,z,z^o$就是整个框架的全部输入，简化可以得到：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620222448091.png" alt="image-20240620222448091" style="zoom: 50%;" />



根据之前5.5.1的计算步骤，由于要计算$f(z^i) \cdot g(z)$，再进行相加，抽象成数学可以得到：



<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620222725543.png" alt="image-20240620222725543" style="zoom:50%;" />

此时的记忆单元Cell 可以作为下一个LSTM的输入，还记得我们最开始把cell的值记为0，也就是上图中的$c^{t-1}$。然后继续下去。

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620222941369.png" alt="image-20240620222941369" style="zoom:50%;" />



但往往，我们不仅把cell里面的$c^t$传入下一个框架的cell，还会考虑下面两个参数传入下一个LSTM作为原始输入的一部分（$x^t$部分），这种方法叫**peephole**

①把每一个的输出$y^t$拷贝为$h^t$也做为输入

②把每一个的输出$c^t$拷贝为$c^t$​也做为输入

即：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620223331036.png" alt="image-20240620223331036" style="zoom: 50%;" />



这种串行的输入也可以抽象为最开始的Simple RNN。这也就是**Multiple-layer LSTM**





### 5.4.4 LSTM是如何对RNN进行改进

也就是LSTM是如何解决梯度消失的问题？

#### 5.4.4.1 梯度消失深层原因

从数学角度来分析RNN的梯度消失原因：理解来自[[LSTM如何解决梯度消失或爆炸的？ - bonelee - 博客园 (cnblogs.com)](https://www.cnblogs.com/bonelee/p/10475453.html)]

<img src="https://pic4.zhimg.com/80/v2-150d5c6b52a05ecb5fd28eda36d9414f_hd.jpg" alt="img" style="zoom: 67%;" />

如上图，是一个每层只有一个神经元的神经网络，且每一层的激活函数为sigmoid，则有：

​                                                  $y_i=\sigma(z_i)=\sigma(w_ix_i+b_i)$

根据反向传播算法有：

​                                  ​
$$
\frac{\delta C}{\delta b_{1}}=\frac{\delta C}{\delta y_{4}}\frac{\delta y_{4}}{\delta z_{4}}\frac{\delta z_{4}}{\delta x_{4}}\frac{\delta x_{4}}{\delta z_{3}}\frac{\delta z_{3}}{\delta x_{3}}\frac{\delta x_{3}}{\delta z_{2}}\frac{\delta z_{2}}{\delta x_{2}}\frac{\delta x_{2}}{\delta z_{1}}\frac{\delta z_{1}}{\delta b_{1}}\\=\frac{\delta C}{\delta y_{4}}\quad(\sigma^{\prime}(z_{4})w_{4})\quad(\sigma^{\prime}(z_{3})w_{3})\quad(\sigma^{\prime}(z_{2})w_{2})\quad(\sigma^{\prime}(z_{1}))
$$
而sigmoid函数的导数公式为： 
$$
S'(x)=\frac{e^{-x}}{(1+e^{-x})^2}=S(x)(1-S(x))
$$
曲线如下：

<img src="https://pic4.zhimg.com/80/v2-ab161e62cd199d6e2e04ad43cf54751b_hd.jpg" alt="img" style="zoom:60%;" />

从上可以看出$\sigma^{\prime}(x)$的最大值为$\frac14$​

通常我们会将权重初始值 $\left|w\right|$初始化为小于1的随机值，因此可以得到
$$
|\sigma'(z_4)w_4|<\frac{1}{4}
$$
随着层数的增多，那么求导结果$\frac{\delta C}{\delta b_{1}}$会越来越小。同理，若将权重初始值 $\left|w\right|$初始化较大，可以得到
$$
|\sigma^{\prime}(z_4)w_4|>1
$$
造成梯度太大（也就是下降的步伐太大），这也是造成梯度爆炸的原因。



#### 5.4.4.2 RNN的梯度消失、爆炸问题

我们给定一个三个时间的RNN单元，如下：

<img src="https://pic1.zhimg.com/80/v2-a9b1bbad6b3044c77e8105851ecf35c4_hd.jpg" alt="img" style="zoom:50%;" />

我们假设最左端的输入$S_{0}$为给定值，且神经元中没有激活函数（便于分析）， 则前向过程如下：
$$
\begin{gathered}
S_{1} =W_xX_1+W_sS_0+b_1\\
S_{2} =W_xX_2+W_sS_1+b_1\\
S_{3} =W_xX_3+W_sS_2+b_1\\
\end{gathered}
$$

$$
O_{1} =W_{o}S_{1}+b_{2} \\
O_{2} =W_{o}S_{2}+b_{2} \\
O_{3} =W_{o}S_{3}+b_{2} 
$$

在$t=3$​时刻，损失函数：
$$
L_3=\frac12(Y_3-O_3)^2
$$
那么如果我们要训练RNN时， 实际上就是是对$W_x,W_s,W_o,b_1,b_2$求偏导， 并不断调整它们以使得 $L_3$尽可能达到最小.那么我们得到以下公式：
$$
\begin{gathered}
\frac{\delta L_{3}}{\delta W_{0}}=\frac{\delta L_{3}}{\delta O_{3}}\frac{\delta O_{3}}{\delta W_{0}} \\
\frac{\delta L_3}{\delta W_x} =\frac{\delta L_3}{\delta O_3}\frac{\delta O_3}{\delta S_3}\frac{\delta S_3}{\delta W_x}+\frac{\delta L_3}{\delta O_3}\frac{\delta O_3}{\delta S_3}\frac{\delta S_3}{\delta S_2}\frac{\delta S_2}{\delta W_x}+\frac{\delta L_3}{\delta O_3}\frac{\delta O_3}{\delta S_3}\frac{\delta S_3}{\delta S_2}\frac{\delta S_2}{\delta S_1}\frac{\delta S_1}{\delta W_x} \\
\frac{\delta L_{3}}{\delta W_{s}} =\frac{\delta L_3}{\delta O_3}\frac{\delta O_3}{\delta S_3}\frac{\delta S_3}{\delta W_s}+\frac{\delta L_3}{\delta O_3}\frac{\delta O_3}{\delta S_3}\frac{\delta S_3}{\delta S_2}\frac{\delta S_2}{\delta W_s}+\frac{\delta L_3}{\delta O_3}\frac{\delta O_3}{\delta S_3}\frac{\delta S_3}{\delta S_2}\frac{\delta S_2}{\delta S_1}\frac{\delta S_1}{\delta W_s} 
\end{gathered}
$$
将上述偏导公式与第三节中的公式比较，我们发现， 随着神经网络层数的加深对$W_o$ 而言并没有什么影响， 而对$W_x,W_s$​会随着时间序列的拉长而产生梯度消失和梯度爆炸问题。

根据上述分析整理一下公式可得， 对于任意时刻$t$对 $W_x,W_s$求偏导的公式为：
$$
\frac{\delta L_t}{\delta W_x}=\sum_{k=0}^t\frac{\delta L_t}{\delta O_t}\frac{\delta O_t}{\delta S_t}(\prod_{j=k+1}^t\frac{\delta S_j}{\delta S_{j-1}})\frac{\delta S_k}{\delta W_x}\\\frac{\delta L_t}{\delta W_s}=\sum_{k=0}^t\frac{\delta L_t}{\delta O_t}\frac{\delta O_t}{\delta S_t}(\prod_{j=k+1}^t\frac{\delta S_j}{\delta S_{j-1}})\frac{\delta S_k}{\delta W_s}
$$
 可以看到，导致梯度消失和爆炸的就在于累乘
$$
\prod_{j=k+1}^t\frac{\delta S_j}{\delta S_{j-1}}
$$
 而加上激活函数后的$S$的表达式为：假设这里用的tanh的激活函数
$$
S_j=tanh(W_xX_j+W_sS_{j-1}+b_1)
$$
那么则有：
$$
\prod_{j=k+1}^t\frac{\delta S_j}{\delta S_{j-1}}=\prod_{j=k+1}^t[tanh'(M_{x,j,s})W_s]\\
M_{x,j,s}=W_xX_j+W_sS_{j-1}+b_1
$$
而在这个公式中，$tanh'(M_{x,j,s})$总是小于1的， 如果$W_{s}$也是一个大于0小于1的值， 那么随着$t$的增大， 上述公式的值越来越趋近于0， 这就导致了梯度消失问题。  如果$W_{s}$很大， 那么随着$t$的增大， 上述公式的值越来越趋近于无穷， 这就导致了梯度爆炸问题。 



#### 5.4.4.3 LSTM的解决方法

从上述中我们知道， RNN产生梯度消失与梯度爆炸的原因就在于 
$$
\prod_{j=k+1}^t\frac{\delta S_j}{\delta S_{j-1}}
$$
如果我们能够将这一坨东西去掉， 就解决掉梯度问题了。 LSTM通过门机制来解决了这个问题。我们先从LSTM的三个门公式出发：

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620140945243.png" alt="image-20240620140945243" style="zoom: 40%;" /><img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620222448091.png" alt="image-20240620222448091" style="zoom: 50%;" />      



假设使用上文讲到的对于输入参数进行改进

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620223331036.png" alt="image-20240620223331036" style="zoom: 50%;" />

此时输入参数(不考虑$c^{t-1}$作为参数)
$$
z^f=W_f\cdot[c^{t-1},h_{t-1},x^t]+b^f\\
z^i=W_i\cdot[c^{t-1},h_{t-1},x^t]+b^i\\
z^o=W_o\cdot[c^{t-1},h_{t-1},x^t]+b^o\\
z=W\cdot[c^{t-1},h_{t-1},x^t]+b
$$
对于三个门单元:
$$
f(z^f)=\sigma(z^f)=\sigma(W_{f}\cdot[h_{t-1},x_{t}]+b_{f})\\
f(z^i)=\sigma(z^i)=\sigma(W_{i}\cdot[h_{t-1},x_{t}]+b_{i})\\
f(z^o)=\sigma(z^o)=\sigma(W_{o}\cdot[h_{t-1},x_{t}]+b_{o})
$$
对于输入单元
$$
g(z)=tanh(z)=tanh(W\cdot[c^{t-1},h_{t-1},x^t]+b)
$$


记忆单元Cell：
$$
c_t=f(z^i)g(z)+c_{t-1}\cdot f(z^f)
$$


隐藏层输出：
$$
y^t=f(z^o)*h(c_t)
$$
若此时函数$h$也采用$tanh$​,则：
$$
y^t=f(z^o)*tanh(c_t)
$$
我们注意到， 首先三个门的激活函数是sigmoid， 这也就意味着这三个门的输出要么接近于0 ， 要么接近于1。这就使得 ，$\frac{\delta c_{t}}{\delta c_{t-1}}=f(z_f)$​​ 是非0即1的。

当门为1时， 梯度能够很好的在LSTM中传递，很大程度上减轻了梯度消失发生的概率， 当门为0时，说明上一时刻的信息对当前时刻没有影响， 我们也就没有必要传递梯度回去来更新参数了。

但是如何确保$\frac{\delta h_{t}}{\delta h_{t-1}}=\frac{\delta y_{t}}{\delta y_{t-1}}$的变化也是非0即1的??

$\frac{\delta h_{t}}{\delta h_{t-1}}=\frac{\delta y_{t}}{\delta y_{t-1}}=f(z_o)$ 不是很这里为什么等于$f(z_o)$









### 5.4.5 LSTM简易代码实现

```python
def LSTMCELL(prev_ct,prev_ht,input):
    combine = prev_ht + input
    ft = forget_layer(combine)
    candidate = candidate_layer(combine)
    it = input_layer(combine)
    ct = prev_ct * ft + candidate * it
    ot = output_layer(combine)
    ht = ot * tanh(ct)
    return ht,ct


ct = [0,0,0]
ht = [0,0,0]
for input in inputs:
   ct,ht = LSTMCELL(ct,ht,input)
```

这个代码实现了一个简单的LSTM(Long Short-Term Memory)单元的前向传播过程。让我来一步步地解释一下:

1. 定义了LSTMCELL函数,它接受三个输入参数:
   - prev_ct: 前一时刻的单元状态(Cell State)
   - prev_ht: 前一时刻的隐藏状态(Hidden State)
   - input: 当前时刻的输入
2. 在函数内部:
   - 首先将前一时刻的隐藏状态和当前输入连接起来,得到一个组合向量`combine`。
   - 然后通过四个不同的层(forget_layer、candidate_layer、input_layer、output_layer)来计算四个关键的LSTM门控:
     - 遗忘门(`ft`)
     - 候选细胞状态(`candidate`)
     - 输入门(`it`)
     - 输出门(`ot`)
   - 利用这四个门控,更新当前时刻的单元状态$ct$和隐藏状态$ht$
     - `ct = prev_ct * ft + candidate * it`
     - `ht = ot * tanh(ct)`
3. 最后,LSTMCELL函数返回更新后的单元状态`ct`和隐藏状态`ht`。

在主程序中:

- 初始化单元状态`ct`和隐藏状态`ht`为全0向量。

- 遍历输入序列`inputs`,对每个输入调用LSTMCELL函数,得到更新后的`ct`和`ht`。

  

