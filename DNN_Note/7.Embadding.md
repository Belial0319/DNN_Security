# 7 Embedding

对于我们在self-attention里面学到的独热编码，但是它需要建立一个所有维度的向量，且维度固定后无法改变，如何改进？使得可以对word进行编码。

## 7.1 Word Embedding简介



<img src="../assets/7.Embadding/image-20240629161833289-1719651534693-19.png" alt="image-20240629161833289" style="zoom:50%;" />

-  1-of-N encoding（one-hot encoding）：每一个词汇都当做一个符号，都用向量来描述，这个方法是有不足的，这样词汇和词汇之间的相关性反映不出来，而且过于稀疏。

- Word Class：建立word class，把有相同性质的word放在同一个 class内，将词汇进行分类，这个方法也比较粗糙，比如动物也分了很多种，不能完全概况。

- Word Embedding：每一个的词汇也用向量来描述，但是每一个维度是一个属性。word embedding，就是找到一个映射或者函数，生成在一个新的空间上的表达。**通俗的翻译可以认为是单词嵌入，就是把X所属空间的单词映射为到Y空间的多维向量**，那么该多维向量相当于嵌入到Y所属空间中。与one-hot编码和word class相比，词嵌入可以将更多的信息塞入更低的维度中。

为什么会出现Word Embedding？

在one hot representation编码的每个单词都是一个维度，彼此independent。

然而每个单词彼此无关这个特点明显不符合我们的现实情况。我们知道大量的单词都是有关。

**语义**：girl和woman虽然用在不同年龄上，但指的都是女性。

**复数**：word和words仅仅是复数和单数的差别。

**时态**：buy和[bought]表达的都是“买”，但发生的时间不同。

所以用one hot representation的编码方式，上面的特性都没有被考虑到。

我们更希望用诸如“语义”，“复数”，“时态”等维度去描述一个单词。每一个维度不再是0或1，而是连续的实数，表示不同的程度。



Word Emebeding是一中无监督学习。通过让模型阅读大量词汇，就可以知道这个embedding的feature vector长什么样子：

<img src="../assets/7.Embadding/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MjMzMjYw,size_16,color_FFFFFF,t_70.png" alt="在这里插入图片描述" style="zoom:50%;" />

就是找一个神经网络，**输入**是一个**词汇**，**输出**该词对应的word embedding 的 **vector**。因为输入就是一堆语言的训练集，但是我们没有标签，不知道embedding长什么样。所以说是无监督的。

## 7.2 基于上下文的两种分析框架

在word embedding中，你要了解词汇的含义可以通过上下文得到,比如两句类似的话：蔡某某宣誓就职；马某某宣誓就职。那么就说明蔡某某和马某某是很类似的两个东西。

### 7.2.1 Count based（基于计算）

Count based。怎么体现两个词汇是类似的？ 词汇$w_1$和 $w_2$常在同一篇文字中出现，那说明 $\mathrm{V}\left(\mathrm{w}_{\mathrm{1}}\right)$和$\mathrm{V}\left(\mathrm{w}_{\mathrm{2}}\right)$比较接近。

<img src="https://img-blog.csdnimg.cn/1143061115a44bb48d73ca1297404b19.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MjMzMjYw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />

原则为：计算内积$\mathrm{V}\left(\mathrm{w}_{\mathrm{1}}\right) \cdot \mathrm{V}\left(\mathrm{w}_{\mathrm{2}}\right)$，计算w1和w2同时在一篇文章中出现的次数$N_{i,j}$，这两个值越接近则说明这两个词义接近。


### 7.2.2 Prediction based（基于预测）

学习一个NN（Neural Network），$W$代表输入的词汇。NN可以根据前一个输入的 $W_{i-1}$,输出下一个可能出现的$W_{i}$​

<img src="../assets/7.Embadding/image-20240629170729232.png" alt="image-20240629170729232" style="zoom:33%;" />

输入是$W_{i-1}$ 的1-of-N encoding，输出下一个词汇 $W_i$ 的概率。

![image-20240629170811944](../assets/7.Embadding/image-20240629170811944.png)

这样可以把第一个hidden layer 的输入$z$拿出来，用这个$z$​代表一个word的embedding feature：


<img src="../assets/7.Embadding/image-20240629171147378.png" alt="image-20240629171147378" style="zoom:50%;" />

是怎么理解是根据上下文(by context)来判定的呢？假设训练数据是：

- 蔡某某（$W_{i-1}$）宣誓就职（$W_i$）
- 马某某（$W_{i-1}$）宣誓就职（$W_i$）

输入之后，都会希望输出“宣誓就职”的概率是高的：
假设输入是蔡某某和马某某，这就必须要通过在第一层参数$W$转换的时候，将马和蔡投影到同一个空间的位置，**当我们考虑输出靠近的时候，就自动考虑输入前就需要靠近了**。就找到这个word embedding。

## 7.2.3 Prediction-based - Sharing Parameters（基于预测的参数共享）

#### 7.2.3.1 原理分析

但可能觉得即使是给了上文一个词，那么下文也很难猜出来，因为这种组合是很多的。怎么办呢？可以把input拓展了到很多个词汇，比如十个。将这些word的排在一起，同时输入到NN中，但是要保证这些word位置固定，比如$W_{i-1}$和$W_{i}$是需要结在一起的。

所谓结在一起就是：**这些词汇的编码向量同一个维度所对应的weight是一样的，**图中用同样颜色表示：

<img src="../assets/7.Embadding/image-20240629171812213.png" alt="image-20240629171812213" style="zoom:50%;" />

为什么呢？一个显而易见的理由是，如果不这么做，同一个word在$W_{i-1}$和$W_{i-2} $的**位置经过变换之后**得到的embedding就会不同，而且可以减少参数量。


$x_{i-1},x_{i-2}$的长度就是$|V|$，$z$的长度就是$|z|$,其中
$$
\mathrm{z=W_1x_{i-2}+W_2x_{i-1}}
$$
权值矩阵$W_1,W_2$都是$|z|\times|V|$的大小，因为$W_1=W_2=W$所以：
$$
\mathrm{z~=~W~(x_{i-2}~+x_{i-1}~)}
$$

#### 7.3.2.2 模型训练

那么怎么才能使得这些$W$是相同的呢？

做法是这样，首先是给一样的初始值，
然后计算偏微分：
$$
\begin{gathered}
w_{i}\leftarrow w_{i}-\eta\frac{\partial C}{\partial w_{i}}-\eta\frac{\partial C}{\partial w_{j}} \\
w_{j}\leftarrow w_{j}-\eta\frac{\partial C}{\partial w_{j}}-\eta\frac{\partial C}{\partial w_{i}} 
\end{gathered}
$$
![在这里插入图片描述](../assets/7.Embadding/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MjMzMjYw,size_16,color_FFFFFF,t_70-1719710403658-21.png)

为了保持相同的更新速度，两者都是减去一样的值，可以确保在训练的过程中永远都是接在一起的。

怎么训练呢？这个训练都是无监督的。收集到一个数据集，输入前两个词，就能输出下一个词

![在这里插入图片描述](../assets/7.Embadding/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MjMzMjYw,size_16,color_FFFFFF,t_70-1719710461790-24.png)

### 7.2.4 Prediction-based - Various Architectures（基于预测的不同架构）

这个模型有很多变形：

#### 7.2.4.1 连续词汇（continuous bag of word，CBOW）
利用前一个和后一个词汇，预测中间的词汇。训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。

![image-20240630092903362](../assets/7.Embadding/image-20240630092903362.png)

#### 7.2.4.2 Skip-gram

![image-20240630092916099](../assets/7.Embadding/image-20240630092916099.png)

利用中间的词汇，预测前一个和后一个词汇。

CBOW（ Continuous Bag of Words）和 Skip-gram 语言模型的工具就是**Word2vec**。

实际上可以通过这个模型发现，这个NN并不深，只有一个线性的hidden layer，作者本人说他自己用的特别好，toolkit很强，不用deep也能做。



## 7.3 Word Embedding 的应用

### 7.3.1 Solving analogies

如果把国家和首都对应在一起，或者把一个动词的三个时态放在一起，这些词汇之间都有一个类似的关系。

![在这里插入图片描述](../assets/7.Embadding/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MjMzMjYw,size_16,color_FFFFFF,t_70-1719711147191-30.png)

如果把word vector两两相减，然后投影到另外一个空间，如果一个词和另一个词有从属关系，那么相减之后的结果会落在邻近区域。

![在这里插入图片描述](../assets/7.Embadding/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MjMzMjYw,size_16,color_FFFFFF,t_70-1719712739046-33.png)

那么从属关系的词之间有这么一个性质，可以有以下的一些推论：

![在这里插入图片描述](../assets/7.Embadding/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MjMzMjYw,size_16,color_FFFFFF,t_70-1719712751912-36.png)

比如：“罗马：意大利 = 柏林：？”，这个可以通过刚刚的性质，由：
$$
\mathrm{V~(~Germany~)}\\\approx\mathrm{V~(~Berlin~)-V~(~Rome~)+V~(~Italy~)}
$$
来得到最接近的vector，那么最后的答案是“德国”。

### 7.3.2 Multi-lingual Embedding

通过中文和英文的两个语料库（corpus）单独地去训练一组word vector，可以发现中文和英文的word vector完全没有任何关系，每个维度对应的含义没有任何联系。

因为，训练word vector靠的是上下文之间的关系，所以如果你的语料库里没有中英文混杂的话，那么机器就无法判断中英文词汇之间的关系。

<img src="../assets/7.Embadding/image-20240630100104290.png" alt="image-20240630100104290" style="zoom:33%;" />

但是如果事先已知一部分中英文的词汇的对应关系，然后再分别得到一组中文vector和英文vector，接下来就可以学习一个模型把事先知道的中英文对应的那些词汇，通过映射到空间中的同一个点（绿色底子汉字或者绿色英文代表已知中英文对应关系的词汇），接下来，如果遇到新的未知的中文或者英文词汇，将英文和中文的word vector投到一个空间后，可以相互靠近，达到类似翻译的效果。


### 7.3.3 Multi-domain Embedding

不仅限于文字，也可以对影像做embedding：

<img src="../assets/7.Embadding/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MjMzMjYw,size_16,color_FFFFFF,t_70-1719712959888-39.png" alt="在这里插入图片描述" style="zoom:67%;" />

在影像分类的时候，模型很难分类新增加的其他类的图片，但是这种Model会找到image和word vector之间的映射关系，所以即使这个model之前没有见过猫的图片，也能将猫图project到“cat”的word vector。就是一种跨域的embedding。

## 7.4  Document Embedding

也可以把一个document变成一个vector：

<img src="../assets/7.Embadding/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MjMzMjYw,size_16,color_FFFFFF,t_70-1719713064150-42.png" alt="在这里插入图片描述" style="zoom:80%;" />

最简单的方法是把一个document变成一个bag of word，然后用自编码器可以学习出这个document的semantic embedding。

<img src="../assets/7.Embadding/image-20240630100531647-1719713132994-45.png" alt="image-20240630100531647" style="zoom:67%;" />

但是这样是不够的，因为词汇顺序代表了很多含义：

<img src="../assets/7.Embadding/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MjMzMjYw,size_16,color_FFFFFF,t_70-1719713182035-47.png" alt="在这里插入图片描述" style="zoom:50%;" />

比如这两个句子换个顺序，可能意思完全相反了。

具体可见：

<img src="../assets/7.Embadding/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3MjMzMjYw,size_16,color_FFFFFF,t_70-1719713197808-50.png" alt="在这里插入图片描述" style="zoom:50%;" />

## 7.5 Embedding总结

Embedding在数学上表示一个maping, f: X -> Y， 也就是一个function，其中该函数是injective（就是我们所说的**单射函数**，每个Y只有唯一的X对应，反之亦然）和structure-preserving (**结构保存**，比如在X所属的空间上X1 < X2,那么映射后在Y所属空间上同理 Y1 < Y2)。那么对于word embedding，就是将单词word映射到另外一个空间，其中这个映射具有injective和structure-preserving的特点。

通俗的翻译可以认为是单词嵌入，**就是把X所属空间的单词映射为到Y空间的多维向量，那么该多维向量相当于嵌入到Y所属空间中，一个萝卜一个坑。**

**word embedding，就是找到一个映射或者函数，生成在一个新的空间上的表达，该表达就是word representation。**

推广开来，还有image embedding, video embedding, 都是一种将源数据映射到另外一个空间

[Word Embedding]([(99+ 封私信 / 80 条消息) 什么是 word embedding? - 知乎 (zhihu.com)](https://www.zhihu.com/question/32275069))

