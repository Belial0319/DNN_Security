# 8 Transformer

Google于2017年6月发布在arxiv上的一篇文章《Attention is all you need》，提出解决sequence to sequence问题的transformer模型，用全self-attention的结构代替了lstm，这也是现在主流的BERT模型的基础。

seq2seq：输入一序列输出一序列

Transformer特点：

- 自注意力机制（Self-Attention）：这是Transformer的核心概念之一，它使模型能够同时考虑输入序列中的所有位置，而不是像循环神经网络（RNN）或卷积神经网络（CNN）一样逐步处理。自注意力机制允许模型根据输入序列中的不同部分来赋予不同的注意权重，从而更好地捕捉语义关系。
- 多头注意力（Multi-Head Attention）：Transformer中的自注意力机制被扩展为多个注意力头，每个头可以学习不同的注意权重，以更好地捕捉不同类型的关系。多头注意力允许模型并行处理不同的信息子空间。
- 堆叠层（Stacked Layers）：Transformer通常由多个相同的编码器和解码器层堆叠而成。这些堆叠的层有助于模型学习复杂的特征表示和语义。
- 位置编码（Positional Encoding）：由于Transformer没有内置的序列位置信息，它需要额外的位置编码来表达输入序列中单词的位置顺序。
- 残差连接和层归一化（Residual Connections and Layer Normalization）：这些技术有助于减轻训练过程中的梯度消失和爆炸问题，使模型更容易训练。
- 编码器和解码器：Transformer通常包括一个编码器用于处理输入序列和一个解码器用于生成输出序列，这使其适用于序列到序列的任务，如机器翻译。



## 8.1 Transformer的粗略结构

**Nx = 6，Encoder block由6个encoder堆叠而成，图中的一个框代表的是一个encoder的内部结构，一个Encoder是由Multi-Head Attention和全连接神经网络Feed Forward Network构成。如下图所示**：

<img src="../assets/8.Transformer/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5ZGK55m95rCU55CDfg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center.png" alt="在这里插入图片描述" style="zoom: 80%;" />

## 8.2 Encoder

输入一排向量，输出一排向量

<img src="../assets/8.Transformer/image-20240630144058539.png" alt="image-20240630144058539" style="zoom:50%;" />

简化后得到：

<img src="../assets/8.Transformer/image-20240630144139571-1719729702331-3.png" alt="image-20240630144139571" style="zoom:50%;" />

这里我们在self-attention中学到计算self-attention，只需要四个输入，但是此时的transformer引入了残差计算，也就是对于书输出的结果，要加上原来的输入，构成residual。

<img src="../assets/8.Transformer/image-20240630144520836.png" alt="image-20240630144520836" style="zoom:50%;" />

也就是在一个self-attention后加上自身。然后需要对得到的residual进行norm。

这里的norm采用的是layer Norm,计算一个向量中所有维度的均值和方差，然后利用均值和方差进行计算得到下一个数据

<img src="../assets/8.Transformer/image-20240630144636109.png" alt="image-20240630144636109" style="zoom:50%;" />

此后按照之前的结果，我们现在需要进行一个全连接层，此时同样进行残差设计，然后再来一个norm



<img src="../assets/8.Transformer/image-20240630144949338.png" alt="image-20240630144949338" style="zoom:50%;" />

最终结果如下：

<img src="../assets/8.Transformer/image-20240630145002263.png" alt="image-20240630145002263" style="zoom:50%;" />

所以回看开始的图片

<img src="../assets/8.Transformer/image-20240630145110445.png" alt="image-20240630145110445" style="zoom: 50%;" />

其中

- **Positional Encoding** ：位置编码
- **Multi-Head attention：**self-attention+Multi-Head
- **Add&Norm：**残差计算加正则化
- **Feed Forwaid：**全连接层，Feed forward 层可以弥补self-attention的非线性（这里不展开）。



## 8.3 Decoder

### 8.3.1 Decoder框架

<img src="../assets/8.Transformer/image-20240630164918768.png" alt="image-20240630164918768" style="zoom:50%;" />

Begin也就是开始，是一个特殊的独热编码

通过Decoder进行输出，类似多分类，在得到第一个输出后，将其作为输入，继续输入到Decoder中。

<img src="../assets/8.Transformer/image-20240630165355608.png" alt="image-20240630165355608" style="zoom:67%;" />

这里会思考，由于输出会变成输入，那么如果中间有了一个错误的输出，再把这个错误的输出当作输入，会不会导致一系列的结果出错？



**Decoder架构**

<img src="../assets/8.Transformer/image-20240630165513231.png" alt="image-20240630165513231" style="zoom: 50%;" />

对于**Encoder vs Decoder**

<img src="../assets/8.Transformer/image-20240630165543249.png" alt="image-20240630165543249" style="zoom:50%;" />

遮住中间的一块，发现歧视两边都差不多，只不过在Multi-head Attention上面多了一个**Masked**

### 8.3.2 Masked self-attention

**Masked：对于self-attention的改进，**

<img src="../assets/8.Transformer/image-20240630165845768.png" alt="image-20240630165845768" style="zoom:50%;" />

对于原本的self-attention的输出$b^{1,2,3,4}$都是由全部的输入$a^{1,2,3,4}$得到。但是这里的Masked attention就是计算$b^i$只用到$a^1,a^2……a^i$,比如说计算$b^1$只用$a^1$，计算$b^2$只用$a^1,a^2$等。

<img src="../assets/8.Transformer/image-20240630170241071.png" alt="image-20240630170241071" style="zoom:50%;" />

<img src="C:\Users\10056\Desktop\科研\学习笔记\assets\image-20240620084019880.png" alt="image-20240620084019880" style="zoom:50%;" />

我们可以回到最开始的attention里面$b^{1}=\sum_i\alpha_{1,i}^{\prime}v^i$​

这里可以在计算式，之用计算前面的$\alpha^{\prime}_{i,j}$即可

<img src="../assets/8.Transformer/image-20240630171626499.png" alt="image-20240630171626499" style="zoom:67%;" />

这里为什么会出现这个Masked?

就是因为在你输入文字或语音的时候是一个一个输进去的，而我们在self-attention计算的时候是一起把$\alpha^i$​并行输入并且计算，所以在识别的时候只能用左边的数据，而不能用右边的。



### 8.3.2 **Autoregressive自回归学习（AT）**

这里有一个问题，decoder的输出长度需要自己指定，像上面写到的，transformer会把上一个输出作为输入，那么怎么让它停下来？

在输出表里面加一个**END中止符号**

<img src="../assets/8.Transformer/image-20240630173820755.png" alt="image-20240630173820755" style="zoom:50%;" />



在输出机器学习后一段文字后，要能够输出END

<img src="../assets/8.Transformer/image-20240630173859123.png" alt="image-20240630173859123" style="zoom:50%;" />

学习的时候，比如说没有声音再输入，就学习到END。然后中止

以上就是**Autoregressive自回归学习（AT）的Decoder**

现在也有**Non-autoregressive （NAT)**

**AT v.s. NAT**

1. AT只有一个启动向量，需要多个步骤才能完成解码；NAT有多个启动向量，只需要一个步骤就能完成解码。

2. 我们不知道输出长度，那怎么确定NAT的输出长度，放多少个BOS呢？答：方法一是用其他的预测模型预测输出长度，方法二是放很多个BOS，输出很长的序列，在end之后的字体就忽略掉。

3. NAT是平行化的，输出长度可控，比AT更加稳定。

4. NAT的效果比AT差，因为multi-modality（多通道）
   

<img src="../assets/8.Transformer/image-20240630174614962.png" alt="image-20240630174614962" style="zoom:50%;" />



### 8.3.3 Cross attention

<img src="../assets/8.Transformer/image-20240630174932195.png" alt="image-20240630174932195" style="zoom:50%;" />

中间遮的这块就叫Cross attention,它是链接Encoder和Decoder之间的桥梁。

<img src="../assets/8.Transformer/image-20240630175044157.png" alt="image-20240630175044157" style="zoom: 80%;" />

这里由Encoder输入两个模组，由之前的Deconder下游任务输出的两个模组（一个进Multi-attention,另外一个进Add&Norm

<img src="../assets/8.Transformer/image-20240630175503699.png" alt="image-20240630175503699" style="zoom:50%;" />

这里输入的两个模块就是$K,V$
$$
\alpha^{\prime}_1=k^1 \cdot q\\
v=\sum\alpha^{\prime}_i \cdot v^i
$$
<img src="../assets/8.Transformer/image-20240630180015331.png" alt="image-20240630180015331" style="zoom:50%;" />

然后就是只有，把上一个的输出作为输入到self-attention（Mask)

<img src="../assets/8.Transformer/image-20240630180137019.png" alt="image-20240630180137019" style="zoom:67%;" />

## 8.4 训练

<img src="../assets/8.Transformer/image-20240630182851376.png" alt="image-20240630182851376" style="zoom:50%;" />

类似于分类问题，对于每一个输出的字与原本设置好的输出样本进行交叉熵计算，使得损失函数越小越好。

<img src="../assets/8.Transformer/image-20240630183144995.png" alt="image-20240630183144995" style="zoom:50%;" />

同时要对句子结尾的段结束符号标记。

**这里会把Decoder的输入，输入的数据为正确答案进行训练**

<img src="../assets/8.Transformer/image-20240630183417618.png" alt="image-20240630183417618" style="zoom:50%;" />

这个就是**Teacher Forcing：**using the ground truth as input.

## 8.5 Seq2Seq 训练 tips

### 8.5.1Copy Mechanism

主要应用场景：

- 聊天机器人

<img src="../assets/8.Transformer/image-20240630183713820.png" alt="image-20240630183713820" style="zoom:50%;" />

也就是对于已经出现的信息，不用再考虑生成，直接复制即可

- 摘要总结

<img src="../assets/8.Transformer/image-20240630183807060.png" alt="image-20240630183807060" style="zoom:50%;" />

也就是主要信息提取，不用全文生成，而是复制

<img src="../assets/8.Transformer/image-20240630183859075.png" alt="image-20240630183859075" style="zoom:50%;" />

### 8.5.2 Guided Attention

例如在语音合成中，

<img src="../assets/8.Transformer/image-20240630184114516.png" alt="image-20240630184114516" style="zoom:50%;" />

黑盒模型，对于每次输出的结果会发现一些奇怪的东西

<img src="../assets/8.Transformer/image-20240630184410440.png" alt="image-20240630184410440" style="zoom:50%;" />

对于attention的计算，它的顺序是随机选取的，而不是从左到右，Guided Attention就是强制顺序进行学习

Monotonic Attention

Location-aware attention

### 8.5.3 Beam Search (束搜索)

<img src="../assets/8.Transformer/image-20240630184739275.png" alt="image-20240630184739275" style="zoom:50%;" />

Beam Search 是一种受限的宽度优先搜索方法，经常用在各种 NLP 生成类任务中，例如机器翻译、对话系统、文本摘要。

#### Beam Search的基础版本

在生成文本的时候，通常需要进行解码操作，贪心搜索 (Greedy Search) 是比较简单的解码。

Beam Search 对贪心搜索进行了改进，扩大了搜索空间，更容易得到全局最优解。

Beam Search 包含一个参数 beam size $k$，表示每一时刻均保留得分最高的 $k $个序列，然后下一时刻用这$ k $个序列继续生成。示意图如下所示：


<img src="../assets/8.Transformer/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4NTU2OTg0,size_16,color_FFFFFF,t_70#pic_center.png" alt="在这里插入图片描述" style="zoom:50%;" />

假设我们生成词表中有三个单词{我，爱，你}。我们设$K=2$。那么我们在第一时刻确定两个候选输出是{我，你}。紧接着我们要考虑第二个输出，具体步骤如下：

- 确定单词“我”为第一时刻输出，并将其作为第二时刻输入，在已知$p(x,我)$的情况下，各个单词的输出概率为3种情况，每个组合的概率为 $P(我|x)P(y_2|x,我)P(我∣x)P(y 2 ∣x,我)$​。
- 同样我们把“你”也作为第二时刻输入，同样也有三种组合。
- 最后我们在六种组合中选择概率最大的三个组合。

接下来要做的重复这个过程，逐步生成单词，直到遇到结束标识符停止。最后得到概率最大的那个生成序列。其概率为：
$$
P(Y|C)=P(y_1|C)P(y_2|C,y_1),...,P(y_6|C,y_1,y_2,y_3,y_4,y_5)
$$
以上就是Beam search算法的思想，当beam size=1时，就变成了贪心算法。